{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e95c03-c84f-4e16-8b24-e49ff0ccce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting protobuf==3.19.0\n",
      "  Downloading protobuf-3.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.2\n",
      "    Uninstalling protobuf-4.23.2:\n",
      "      Successfully uninstalled protobuf-4.23.2\n",
      "Successfully installed protobuf-3.19.0\n"
     ]
    }
   ],
   "source": [
    "#!python -m pip install --upgrade pip\n",
    "#!pip -q install --upgrade transformers\n",
    "#!pip -q install --upgrade accelerate\n",
    "#!pip -q install --upgrade torch\n",
    "#!pip -q install --upgrade sentencepiece\n",
    "!pip install --upgrade protobuf==3.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70caaf67-79b6-4ae7-8db9-70cf6ecebf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "080c6f44-2197-49bc-8f8c-0700f3b55ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# shutil.rmtree('llama/hf')\n",
    "#!mkdir llama/hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b91edb-a4c7-4bcf-a379-2aff92edaf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil -m cp -r gs://calibragpt/llama/65B llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2908a37-0da8-491d-b791-dfeeb8f2eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil cp gs://calibragpt/llama/llama.sh llama\n",
    "#!gsutil cp gs://calibragpt/llama/tokenizer.model llama\n",
    "#!gsutil cp gs://calibragpt/llama/tokenizer_checklist.chk llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8504eb2e-cd95-4ffe-9ef0-b35ee7ca239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65B  hf  llama.sh  tokenizer.model  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "!ls llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca4f929e-b893-4ecf-88da-1377275d4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "738d6e94-01ba-48ae-99cf-44c6aa10805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-weight-transfer.ipynb  llama  transformers\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52f0649b-52e3-49c0-aff8-c181f458f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-14 05:46:21.185361: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 05:46:21.344320: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: No preset parameters were found for the device that Open MPI\n",
      "detected:\n",
      "\n",
      "  Local host:            209-20-158-212\n",
      "  Device name:           mlx5_0\n",
      "  Device vendor ID:      0x02c9\n",
      "  Device vendor part ID: 4122\n",
      "\n",
      "Default device parameters will be used, which may result in lower\n",
      "performance.  You can edit any of the files specified by the\n",
      "btl_openib_device_param_files MCA parameter to set values for your\n",
      "device.\n",
      "\n",
      "NOTE: You can turn off this warning by setting the MCA parameter\n",
      "      btl_openib_warn_no_device_params_found to 0.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "No OpenFabrics connection schemes reported that they were able to be\n",
      "used on a specific port.  As such, the openib BTL (OpenFabrics\n",
      "support) will be disabled for this port.\n",
      "\n",
      "  Local host:           209-20-158-212\n",
      "  Local device:         mlx5_0\n",
      "  Local port:           1\n",
      "  CPCs attempted:       udcm\n",
      "--------------------------------------------------------------------------\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "Saving a LlamaTokenizerFast to llama/hf.\n"
     ]
    }
   ],
   "source": [
    "!python transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n",
    "    --input_dir llama --model_size tokenizer_only --output_dir llama/hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfbf9442-0e43-4f80-bd37-3a90fc25489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-weight-transfer.ipynb  llama  transformers\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70f2d565-977a-4db1-ad70-1a93e9f259d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='llama/hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"llama/hf\")\n",
    "# model = LlamaForCausalLM.from_pretrained(\"llama/hf\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8003e5f-e6ab-44d6-8085-4b832fbf0d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://llama/hf/special_tokens_map.json [Content-Type=application/json]...\n",
      "Copying file://llama/hf/pytorch_model-00008-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://llama/hf/tokenizer.json [Content-Type=application/json]...\n",
      "Copying file://llama/hf/pytorch_model-00012-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/tokenizer_config.json [Content-Type=application/json]...\n",
      "Copying file://llama/hf/pytorch_model-00003-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/pytorch_model-00009-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/pytorch_model-00006-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/pytorch_model.bin.index.json [Content-Type=application/json]...\n",
      "Copying file://llama/hf/pytorch_model-00005-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/pytorch_model-00001-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/pytorch_model-00007-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/pytorch_model-00010-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/tokenizer.model [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/pytorch_model-00002-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/pytorch_model-00014-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/pytorch_model-00011-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/pytorch_model-00013-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/pytorch_model-00004-of-00014.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://llama/hf/generation_config.json [Content-Type=application/json]...\n",
      "Copying file://llama/hf/config.json [Content-Type=application/json]...\n",
      "| [21/21 files][121.6 GiB/121.6 GiB] 100% Done 160.3 MiB/s ETA 00:00:00         \n",
      "Operation completed over 21 objects/121.6 GiB.                                   \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r llama/hf gs://calibragpt/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444dbbf1-dcd9-4ef1-8e69-d49a767a1ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
