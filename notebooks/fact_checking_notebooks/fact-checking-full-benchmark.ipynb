{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am_Ubq6xTaBz"
   },
   "source": [
    "# Run Fact Checking Benchmark\n",
    "This notebook contains the code to run the CalibraGPT fact checking benchmark. Fact checking is accomplished by probing whether factual statements are predicted at a higher probability compared to paired counterfactual statements. We will be using the [CalibraGPT/Fact_Checking](https://huggingface.co/datasets/CalibraGPT/Fact_Checking) dataset. See the Repo's [README](https://github.com/daniel-furman/Capstone#model-families-tested) for compatible models and more information.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/daniel-furman/Capstone/blob/main/notebooks/fact_checking_notebooks/fact-checking-full-benchmark.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uccv2X7WeJGv"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4gro-sOZz-O"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/daniel-furman/Capstone.git\n",
    "!pip install -r /content/Capstone/requirements.txt\n",
    "#!pip install -r /content/Capstone/requirements_llama.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yjnEaRtKd8L"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-a72Ac0RbMnb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from argparse import Namespace\n",
    "from transformers import set_seed\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.chdir(\"/content/Capstone/src/fact_checking_scripts\")\n",
    "from compare_models import compare_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gqw4gExYvhHK"
   },
   "source": [
    "## Configure Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCHXtnnfvjGm"
   },
   "outputs": [],
   "source": [
    "# args config for running the benchmark\n",
    "args = Namespace(\n",
    "    model=\"/content/drive/MyDrive/Colab Files/llama/LLaMA/int8/llama-13b\",\n",
    "    language=\"de\",\n",
    ")\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPCmhNRjKPlu"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enPdtlx-aH3k"
   },
   "outputs": [],
   "source": [
    "# ensure GPU access\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"Change runtime type to include a GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGUAvaJbJ-1n"
   },
   "outputs": [],
   "source": [
    "# set warning level\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJyKtP3JrtPS"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biZ8sWXcKRsG"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70WlwWC7Gl9m"
   },
   "outputs": [],
   "source": [
    "# run the fact checking benchmark\n",
    "print(\"Running the fact_checking benchmark...\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# change the below to work on a list of (\"english\", \"en\") pairs\n",
    "# load in the dataset corresponding to the input language\n",
    "supported_languages = [\n",
    "    (\"english\", \"en\"),\n",
    "    (\"french\", \"fr\"),\n",
    "    (\"spanish\", \"es\"),\n",
    "    (\"chinese\", \"zh\"),\n",
    "    (\"japanese\", \"ja\"),\n",
    "    (\"german\", \"de\"),\n",
    "]\n",
    "\n",
    "dataset_bool = False\n",
    "for lang_arr in supported_languages:\n",
    "    if (args.language.lower() == lang_arr[0]) or (args.language.lower() == lang_arr[1]):\n",
    "        dataset = load_dataset(\n",
    "            \"CalibraGPT/Fact_Checking\", split=lang_arr[0].capitalize()\n",
    "        )\n",
    "        dataset_bool = True\n",
    "\n",
    "if not dataset_bool:\n",
    "    raise Exception(\"Language not supported.\")\n",
    "\n",
    "# check the input model is compatible\n",
    "compatible_model_prefixes = [\n",
    "    \"flan\",\n",
    "    \"t5\",\n",
    "    \"pythia\",\n",
    "    \"gpt\",\n",
    "    \"opt\",\n",
    "    \"llama\",\n",
    "    \"roberta\",\n",
    "    \"bert\",\n",
    "    \"bloom\",\n",
    "]\n",
    "\n",
    "model_supported = False\n",
    "for model_prefix in compatible_model_prefixes:\n",
    "    if model_prefix in args.model.lower():\n",
    "        model_supported = True\n",
    "\n",
    "if not model_supported:\n",
    "    raise Exception(\"Model not supported.\")\n",
    "\n",
    "# create a config for running the pipeline\n",
    "config = {\n",
    "    \"models\": [args.model],\n",
    "    \"input_information\": dataset,\n",
    "    \"verbosity\": False,\n",
    "}\n",
    "\n",
    "# run the contrastive knowledge assessment function\n",
    "# logs saved at './content/logging/'\n",
    "score_dicts = compare_models(\n",
    "    config[\"models\"], config[\"input_information\"], config[\"verbosity\"]\n",
    ")\n",
    "\n",
    "# print the summary results\n",
    "print(f\"\\nScore dict summary:\\n{score_dicts[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJtfHqlG7kJr"
   },
   "outputs": [],
   "source": [
    "# colab specific code for saving logs to Drive after running\n",
    "# change to path in your own drive\n",
    "\n",
    "os.chdir(\"../../../\")\n",
    "log = glob.glob(\"/content/logging/*json\")[0]\n",
    "log_name = log.split(\"/\")[-1]\n",
    "log_new_path = f\"/content/drive/MyDrive/Colab Files/cka_benchmark_logs/{log_name}\"\n",
    "!cp {log} '{log_new_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SN14izjjr3eX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
