{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache calinet probing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/calinet_probing_data_original/probing_data_trex_500each.json', 'r') as f:\n",
    "    data_calinet = json.load(f)\n",
    "    starter_df = pd.DataFrame(list(data_calinet['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter df\n",
    "starter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of these have to do with fact id 1\n",
    "# the sentences are formed in this format...\n",
    "# the start of a factual sentence, involving the subject\n",
    "# and then two possibilities: one true and one false?\n",
    "# storing these, then, we should do something like\n",
    "# sentence stem | correct | incorrect\n",
    "# and we can strip out the <extra_id_x> parts\n",
    "# to keep it model agnostic\n",
    "starter_df['sentences'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create containers to hold our clean data\n",
    "sentence_stems = []\n",
    "correct = []\n",
    "incorrect = []\n",
    "fact_ids = []\n",
    "relations = []\n",
    "subjects = []\n",
    "objects = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in starter_df.iterrows():\n",
    "    sentence_list = row['sentences']\n",
    "    for entry in sentence_list:\n",
    "        \n",
    "        # minor cleanup \n",
    "        cleaned_stem = entry[0].replace(\"<extra_id_0>\", \"[BLANK]\").strip()\n",
    "        cleaned_correct = entry[1].replace(\"<extra_id_0>\", \"\").replace(\"<extra_id_1>\", \"\").strip()\n",
    "        cleaned_incorrect = entry[2].replace(\"<extra_id_0>\", \"\").replace(\"<extra_id_1>\", \"\").strip()\n",
    "        \n",
    "        # grab sub<->obj\n",
    "        subjects_and_objects = pd.json_normalize(row['triplet'])\n",
    "        subjects.append(subjects_and_objects.sub_label.values[0])\n",
    "        objects.append(subjects_and_objects.obj_label.values[0])\n",
    "        \n",
    "        # commit \n",
    "        sentence_stems.append(cleaned_stem)\n",
    "        correct.append(cleaned_correct)\n",
    "        incorrect.append(cleaned_incorrect)\n",
    "        fact_ids.append(row['fact_id'])\n",
    "        relations.append(row['relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert(len(sentence_stems) ==\n",
    "       len(correct) ==\n",
    "       len(incorrect) ==\n",
    "       len(fact_ids) ==\n",
    "       len(relations) ==\n",
    "      len(subjects) ==\n",
    "      len(objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into big df\n",
    "trex_df = pd.DataFrame({'fact_id': fact_ids,\n",
    "                        'relation': relations, 'subject': subjects,\n",
    "                        'object': objects, 'stem': sentence_stems, 'true': correct,\n",
    "                        'false': incorrect})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full df\n",
    "trex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out initial df\n",
    "trex_df.to_json('../data/calinet_probing_data_original/calinet_trex_full_data.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put false inputs into a list\n",
    "# with open('../data/calinet_probing_data_original/calinet_trex_full_data.json', 'r') as f:\n",
    "    # data_calinet = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many stems end in [BLANK]? -> 50451, or about 1/3.\n",
    "c = 0\n",
    "for stem in trex_df['stem']:\n",
    "    if stem.endswith(\"[BLANK].\"):\n",
    "        c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_causal_compatibility(stem):\n",
    "    return stem.endswith(\"[BLANK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_stem(stem):\n",
    "    if stem.endswith(\"[BLANK].\"):\n",
    "        return stem[0: len(stem)-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_causal_df = trex_df[trex_df.apply(lambda x: check_for_causal_compatibility(x.stem), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_causal_df = trex_causal_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_stems = trex_causal_df.apply(lambda x: trim_stem(x.stem), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_causal_df['stem'] = list(trimmed_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only about 20% of the calinet data is 'unique' knowledge, since they used paraphrases to calibrate\n",
    "len(trex_causal_df['fact_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before sampling, attach arbitrary counter ID, to then track who gets removed\n",
    "trex_causal_df['calibra_id'] = range(50451)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_causal_subset = trex_causal_df.groupby('fact_id').apply(lambda x: x.sample(1, random_state=42)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(trex_causal_subset.shape[0] == len(trex_causal_df['fact_id'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_causal_subset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_causal_subset.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_ids = {}\n",
    "removed_counterfacts = {}\n",
    "for c_id in trex_causal_df['calibra_id']:\n",
    "    if c_id not in trex_causal_subset['calibra_id'].values:\n",
    "        fact_id = trex_causal_df[trex_causal_df['calibra_id'] == c_id]['fact_id'].values[0]\n",
    "        counterfact = trex_causal_df[trex_causal_df['calibra_id'] == c_id]['false'].values[0]\n",
    "        removed_ids[str(c_id)] = int(fact_id)\n",
    "        if str(fact_id) in removed_counterfacts:\n",
    "            removed_counterfacts[str(fact_id)].append(counterfact)\n",
    "        else:\n",
    "            removed_counterfacts[str(fact_id)] = [counterfact]\n",
    "\n",
    "# did we remove as many rows as eq to the difference between the full calinet dataset row number and the unique count?\n",
    "assert(len(removed_ids) == trex_causal_df.shape[0] - len(trex_causal_df['fact_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are essentially the extra false things we can test against\n",
    "# that are still worth keeping\n",
    "c = 0\n",
    "for k, v in removed_counterfacts.items():\n",
    "    print(k, v)\n",
    "    c+=1\n",
    "    if c == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop extraneous calibra_id column \n",
    "trex_causal_subset.drop(['calibra_id'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are some fact_id's that only have 1 row\n",
    "# since we did pull stuff out based on our left to right requirement\n",
    "trex_causal_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removed_counterfacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_falses = {}\n",
    "for k, v in removed_counterfacts.items():\n",
    "    subset_false = trex_causal_subset[trex_causal_subset['fact_id'] == int(k)].false.values[0]\n",
    "    full_falses[k] = v\n",
    "    full_falses[k].append(subset_false)\n",
    "\n",
    "print(len(full_falses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in full_falses.items():\n",
    "    print(k,v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_false_column(fact_id, false_val, full_false_dict=full_falses):\n",
    "    if str(fact_id) in full_false_dict:\n",
    "        return full_false_dict[str(fact_id)]\n",
    "    else:\n",
    "        return [false_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_falses = list(trex_causal_subset.apply(lambda x: replace_false_column(x.fact_id, x.false), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(replaced_falses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_falses[:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_causal_subset['false'] = replaced_falses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_causal_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_causal_subset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {}\n",
    "trex_list = trex_causal_subset.to_dict('records')\n",
    "for i, entry in enumerate(trex_list):\n",
    "    output_dict[i] = trex_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs = 0\n",
    "for x, y in output_dict.items():\n",
    "    output_dict[x] = y \n",
    "    output_dict[x]['false'] = list(set(y['false']))\n",
    "    \n",
    "    num_pairs += len(output_dict[x]['false'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out cleaned/formatted df\n",
    "with open(\n",
    "    f\"../data/calinet_input_information.json\", \"w\"\n",
    ") as outfile:\n",
    "    json.dump(output_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of curiosity, which relation templates persist in the cleaned, 'causal friendly' set...\n",
    "trex_causal_df['relation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache ROME counterfact data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/rome_counterfact_original/counterfact.json', 'r') as f:\n",
    "    data_rome = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_rome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rome_input_information = {}\n",
    "\n",
    "for i in range(len(data_rome)):\n",
    "    stem = data_rome[i]['requested_rewrite']['prompt'].replace('{}', data_rome[i]['requested_rewrite']['subject'])\n",
    "    \n",
    "    data_rome_input_information[str(i)] = {\n",
    "        \"stem\": stem,\n",
    "        \"true\": data_rome[i]['requested_rewrite']['target_true']['str'],\n",
    "        \"false\": [data_rome[i]['requested_rewrite']['target_new']['str']],\n",
    "        \"case_id\":  data_rome[i]['case_id']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_rome_input_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    f\"../data/rome_counterfact_input_information.json\", \"w\"\n",
    ") as outfile:\n",
    "    json.dump(data_rome_input_information, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/calinet_input_information.json', 'r') as f:\n",
    "    data_calinet = json.load(f)\n",
    "\n",
    "with open('../data/rome_counterfact_input_information.json', 'r') as f:\n",
    "    data_rome= json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_calinet\n",
    "#data_rome\n",
    "\n",
    "mixed_itr = 0\n",
    "mixed_df = {}\n",
    "\n",
    "for x, y in data_calinet.items():\n",
    "    y['dataset_original'] = 'calinet_input_information'\n",
    "    mixed_df[str(mixed_itr)] = y\n",
    "\n",
    "    mixed_itr+=1\n",
    "\n",
    "for x, y in data_rome.items():\n",
    "    y['dataset_original'] = 'rome_counterfact_input_information'\n",
    "    mixed_df[str(mixed_itr)] = y\n",
    "    mixed_itr+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrs = 0\n",
    "for x, y in mixed_df.items():\n",
    "    itrs += 1\n",
    "\n",
    "print(f'The number of items in mixed_df is {itrs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate stem, fact, counterfact pairs across the dataset:\n",
    "\n",
    "pairs_list = []\n",
    "for x, y in mixed_df.items():\n",
    "    for itr in range(len(y['false'])):\n",
    "        pairs = [y['stem'] + ' ' + y['true'] + ' ' + y['false'][itr]]\n",
    "        pairs_list.append(pairs)\n",
    "\n",
    "print(f'The number of [stem + fact + counterfact] trios in mixed_df is {len(pairs_list)}')\n",
    "# num duplicates \n",
    "num_dup = len(pairs_list) - len(np.unique(np.array(pairs_list)))\n",
    "print(f'The number of duplicated [stem + fact + counterfact] trios in mixed_df is {num_dup}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 2 duplicates\n",
    "\n",
    "pairs_list = []\n",
    "dup_itr = 1\n",
    "for x, y in mixed_df.items():\n",
    "    for itr in range(len(y['false'])):\n",
    "        pairs = [y['stem'] + ' ' + y['true'] + ' ' + y['false'][itr]]\n",
    "        if pairs in pairs_list:\n",
    "            print(x, dup_itr, pairs)\n",
    "            dup_itr+=1\n",
    "        pairs_list.append(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard code remove the above\n",
    "mixed_df['10107']['false'] = ['Bulgaria']\n",
    "mixed_df['10107']\n",
    "del mixed_df['25020']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates again\n",
    "\n",
    "pairs_list = []\n",
    "for x, y in mixed_df.items():\n",
    "    for itr in range(len(y['false'])):\n",
    "        pairs = [y['stem'] + ' ' + y['true'] + ' ' + y['false'][itr]]\n",
    "        pairs_list.append(pairs)\n",
    "\n",
    "print(f'The number of [stem + fact + counterfact] trios in mixed_df is {len(pairs_list)}')\n",
    "# num duplicates \n",
    "num_dup = len(pairs_list) - len(np.unique(np.array(pairs_list)))\n",
    "print(f'The number of duplicated [stem + fact + counterfact] trios in mixed_df is {num_dup}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates in stem + fact pairs -> combine duplicates to one item\n",
    "\n",
    "x_list = []\n",
    "pairs_list = []\n",
    "pairs_dup_list = []\n",
    "for x, y in mixed_df.items():\n",
    "    pairs = [y['stem'] + ' ' + y['true']]\n",
    "    if pairs in pairs_list:\n",
    "        x_list.append(x)\n",
    "        pairs_dup_list.append(pairs)\n",
    "\n",
    "    pairs_list.append(pairs)\n",
    "\n",
    "print(f'The number of [stem + fact] pairs in mixed_df is {len(pairs_list)}')\n",
    "# num duplicates \n",
    "num_dup = len(pairs_list) - len(np.unique(np.array(pairs_list)))\n",
    "print(f'The number of duplicated [stem + fact] pairs in mixed_df is {num_dup}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in x_list:\n",
    "    element = mixed_df[x]\n",
    "    pairs = [element['stem'] + ' ' + element['true']]\n",
    "\n",
    "    for x_2, y in mixed_df.items():\n",
    "        pairs_2 = [y['stem'] + ' ' + y['true']]\n",
    "        if (pairs == pairs_2) and (x != x_2):\n",
    "            # extend x_2 counterfacts list with x counterfacts\n",
    "            # grab the set so they are all unique items\n",
    "            # del mixed_df[x] below\n",
    "            mixed_df[x_2]['false'].extend(mixed_df[x]['false'])\n",
    "            mixed_df[x_2]['false'] = list(set(mixed_df[x_2]['false']))\n",
    "\n",
    "\n",
    "for x in x_list:\n",
    "    del mixed_df[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates in stem + fact pairs again\n",
    "# should be 0\n",
    "\n",
    "x_list = []\n",
    "pairs_list = []\n",
    "pairs_dup_list = []\n",
    "for x, y in mixed_df.items():\n",
    "    pairs = [y['stem'] + ' ' + y['true']]\n",
    "    if pairs in pairs_list:\n",
    "        x_list.append(x)\n",
    "        pairs_dup_list.append(pairs)\n",
    "\n",
    "    pairs_list.append(pairs)\n",
    "\n",
    "print(f'The number of [stem + fact] pairs in mixed_df is {len(pairs_list)}')\n",
    "# num duplicates \n",
    "num_dup = len(pairs_list) - len(np.unique(np.array(pairs_list)))\n",
    "print(f'The number of duplicated [stem + fact] pairs in mixed_df is {num_dup}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final numbers\n",
    "pairs_list = []\n",
    "for x, y in mixed_df.items():\n",
    "    for itr in range(len(y['false'])):\n",
    "        pairs = [y['stem'] + ' ' + y['true'] + ' ' + y['false'][itr]]\n",
    "        pairs_list.append(pairs)\n",
    "\n",
    "print(f'The number of [stem + fact + counterfact] trios in mixed_df is {len(pairs_list)}')\n",
    "\n",
    "x_list = []\n",
    "pairs_list = []\n",
    "for x, y in mixed_df.items():\n",
    "    pairs = [y['stem'] + ' ' + y['true']]\n",
    "    pairs_list.append(pairs)\n",
    "\n",
    "print(f'The number of [stem + fact] pairs in mixed_df is {len(pairs_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final count of duplicates fact_id and case_id\n",
    "case_id_list = []\n",
    "fact_id_list = []\n",
    "\n",
    "for x, y in mixed_df.items():\n",
    "    try:\n",
    "        case_id_list.append(y['case_id'])\n",
    "    except:\n",
    "        fact_id_list.append(y['fact_id'])\n",
    "\n",
    "print(f'The number of duplicated case_ids is {len(case_id_list) - len(list(set(case_id_list)))}')\n",
    "print(f'The number of duplicated fact_ids is {len(fact_id_list) - len(list(set(fact_id_list)))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the final mixed json to file\n",
    "\n",
    "with open(\n",
    "    f\"../data/calibragpt_full_input_information.json\", \"w\"\n",
    ") as outfile:\n",
    "    json.dump(mixed_df, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test load of mixed json\n",
    "\n",
    "with open(\n",
    "    f\"../data/calibragpt_full_input_information.json\", \"r\"\n",
    ") as outfile:\n",
    "    mixed_df = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final numbers test 2\n",
    "pairs_list = []\n",
    "for x, y in mixed_df.items():\n",
    "    for itr in range(len(y['false'])):\n",
    "        pairs = [y['stem'] + ' ' + y['true'] + ' ' + y['false'][itr]]\n",
    "        pairs_list.append(pairs)\n",
    "\n",
    "print(f'The number of [stem + fact + counterfact] trios in mixed_df is {len(pairs_list)}')\n",
    "\n",
    "x_list = []\n",
    "pairs_list = []\n",
    "for x, y in mixed_df.items():\n",
    "    pairs = [y['stem'] + ' ' + y['true']]\n",
    "    pairs_list.append(pairs)\n",
    "\n",
    "print(f'The number of [stem + fact] pairs in mixed_df is {len(pairs_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update mixed_df to have all info for rome then write that out. \n",
    "mixed_df = pd.DataFrame.from_dict(mixed_df).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rome info to look at:\n",
    "with open('../data/rome_counterfact_original/counterfact.json', 'r') as f:\n",
    "    data_rome = json.load(f)\n",
    "    rome_df = pd.DataFrame.from_dict(data_rome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3/20 data frame cleanup\n",
    "rome_df.head()\n",
    "\n",
    "rome_subjects = {}\n",
    "rome_objects = {}\n",
    "rome_relations = {}\n",
    "\n",
    "for i, rewrite in enumerate(rome_df['requested_rewrite']):\n",
    "    rome_subjects[i] = rewrite['subject']\n",
    "    rome_objects[i] = rewrite['target_true']['str']\n",
    "    rome_relations[i] = rewrite['relation_id']\n",
    "\n",
    "assert(len(rome_subjects) == len(rome_objects) == len(rome_relations) == rome_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = []\n",
    "objects = []\n",
    "ids = []\n",
    "relations = []\n",
    "\n",
    "for row in mixed_df.iterrows():\n",
    "    if row[1]['dataset_original'] == 'calinet_input_information':\n",
    "        subjects.append(row[1]['subject'])\n",
    "        objects.append(row[1]['object'])\n",
    "        relations.append(row[1]['relation'])\n",
    "        ids.append('calinet_' + str(row[1]['fact_id']))\n",
    "    if row[1]['dataset_original'] == 'rome_counterfact_input_information':\n",
    "        # get case id\n",
    "        case_id = row[1]['case_id']\n",
    "        \n",
    "        # get subject\n",
    "        subjects.append(rome_subjects[case_id])\n",
    "        # get object\n",
    "        objects.append(rome_objects[case_id])\n",
    "        # get relation\n",
    "        relations.append(rome_relations[case_id])\n",
    "        ids.append('rome_' + str(case_id))\n",
    "\n",
    "assert(len(subjects) == len(objects) == len(ids) == len(relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df['subject'] = subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df['object'] = objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df['relation'] = relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df['dataset_id'] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df.drop(['fact_id', 'case_id', 'dataset_original'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(not mixed_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file as .csv\n",
    "mixed_df.to_csv('../data/calibragpt_full_input_information_3_20_23.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "301faebbd5cea7fd4466786a19f1bea9d8baf657aaca95ef39840c46b8697603"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
