{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f71ad4",
   "metadata": {
    "id": "94f71ad4"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc63c4d",
   "metadata": {
    "id": "fcc63c4d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import time\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from pycountry_convert import country_alpha2_to_continent_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa36919e",
   "metadata": {
    "id": "fa36919e"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede8e4f",
   "metadata": {
    "id": "9ede8e4f"
   },
   "outputs": [],
   "source": [
    "wiki_data = pd.read_csv(\"../../data/wikidata/wikidata-property-list.csv\")\n",
    "wiki_data = wiki_data[[\"Title\", \"ID\", \"Datatype\", \"Description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d4cb5",
   "metadata": {
    "id": "1e1d4cb5"
   },
   "outputs": [],
   "source": [
    "code_to_lang_dict = {\n",
    "    \"bg\": \"Bulgarian\",\n",
    "    \"ca\": \"Catalan\",\n",
    "    \"cs\": \"Czech\",\n",
    "    \"da\": \"Danish\",\n",
    "    \"de\": \"German\",\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"fr\": \"French\",\n",
    "    \"hr\": \"Croatian\",\n",
    "    \"hu\": \"Hungarian\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"nl\": \"Dutch\",\n",
    "    \"pl\": \"Polish\",\n",
    "    \"pt\": \"Portuguese\",\n",
    "    \"ro\": \"Romanian\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"sl\": \"Slovenian\",\n",
    "    \"sr\": \"Serbian\",\n",
    "    \"sv\": \"Swedish\",\n",
    "    \"uk\": \"Ukrainian\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d61967",
   "metadata": {
    "id": "89d61967"
   },
   "outputs": [],
   "source": [
    "lang_to_code_dict = {v: k for k, v in code_to_lang_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216b098",
   "metadata": {
    "id": "4216b098"
   },
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "results_dict[\"language\"] = []\n",
    "results_dict[\"relation\"] = []\n",
    "results_dict[\"percentage change\"] = []\n",
    "results_dict[\"new ratio of rows\"] = []\n",
    "results_dict[\"old ratio of rows\"] = []\n",
    "\n",
    "hf_df = datasets.load_dataset(\"CalibraGPT/Fact-Completion\")\n",
    "file_names = glob.glob(\"../../data/result_logs/llama-30b/error-analysis/*.csv\")\n",
    "\n",
    "# confirm grabbing data correctly against LLaMa figure\n",
    "# uncomment print statement at end of for loop to see\n",
    "results_dfs = []\n",
    "count = 0\n",
    "for file in file_names:\n",
    "    language = file.split(\".csv\")[0].split(\"-\")[-1].capitalize()\n",
    "    error_df = pd.read_csv(file)\n",
    "    full_hf_df = hf_df[file.split(\".csv\")[0].split(\"-\")[-1].capitalize()]\n",
    "    full_hf_df = full_hf_df.to_pandas()\n",
    "\n",
    "    # stem is in both\n",
    "    # dataset id is in both\n",
    "    # to see if the model got something wrong, see if the dataset id in the full df is in the error\n",
    "    error_ids = list(error_df[\"dataset_id\"])\n",
    "    correct = []\n",
    "    counts = []\n",
    "    relation_names = []\n",
    "    for row in full_hf_df.iterrows():\n",
    "        # track counts\n",
    "        count += 1\n",
    "        counts.append(count)\n",
    "        # track errors\n",
    "        correct.append(False) if row[1][\"dataset_id\"] in error_ids else correct.append(\n",
    "            True\n",
    "        )\n",
    "        # track relation titles\n",
    "        relation_id = int(row[1].relation[1:])\n",
    "        relation_title = list(wiki_data[wiki_data[\"ID\"] == relation_id][\"Title\"])[0]\n",
    "        relation_names.append(relation_title)\n",
    "\n",
    "    # append result to full df\n",
    "    full_hf_df[\"correct\"] = correct\n",
    "    # append language to full df\n",
    "    full_hf_df[\"language\"] = [language] * full_hf_df.shape[0]\n",
    "    # append language code to full df\n",
    "    lang_code = lang_to_code_dict[language]\n",
    "    full_hf_df[\"lang_code\"] = [lang_code] * full_hf_df.shape[0]\n",
    "    # append relation title to full df\n",
    "    full_hf_df[\"relation_title\"] = relation_names\n",
    "    # also append an arbitrary id to have unique val for each row\n",
    "    full_hf_df[\"analysis_id\"] = counts\n",
    "\n",
    "    results_dfs.append(full_hf_df)\n",
    "\n",
    "results_df = pd.concat(results_dfs)\n",
    "assert results_df.shape[0] == count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d4896",
   "metadata": {
    "id": "ca1d4896"
   },
   "source": [
    "## More Cleanup to Ensure that we have access to Subjects across langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e88e4",
   "metadata": {
    "id": "c50e88e4"
   },
   "outputs": [],
   "source": [
    "# mapping between dataset id and the english form of a subject\n",
    "dataset_id_to_eng_subject = {}\n",
    "for row in results_df.iterrows():\n",
    "    if row[1].language == \"English\":\n",
    "        if row[1].dataset_id not in dataset_id_to_eng_subject:\n",
    "            dataset_id_to_eng_subject[row[1].dataset_id] = row[1].subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fa81f",
   "metadata": {
    "id": "f87fa81f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entities = {}\n",
    "for row in results_df.iterrows():\n",
    "    # gather helpful row level data\n",
    "    # the subject\n",
    "    subject = row[1].subject\n",
    "    # whether the model got it right\n",
    "    val = row[1].correct\n",
    "    # the dataset id\n",
    "    dataset_id = row[1].dataset_id\n",
    "    # the english version of the subject\n",
    "    english_subject = dataset_id_to_eng_subject[dataset_id]\n",
    "\n",
    "    # commit it to our tracking dict\n",
    "    if english_subject not in entities:\n",
    "        entities[english_subject] = {\n",
    "            \"correct\": 0,\n",
    "            \"incorrect\": 0,\n",
    "            \"langs\": {},\n",
    "            \"alternate_forms\": {},\n",
    "            \"dataset_ids\": set(),\n",
    "        }\n",
    "\n",
    "    # counter of correct/incorrect for that subject\n",
    "    if val:\n",
    "        entities[english_subject][\"correct\"] += 1\n",
    "    else:\n",
    "        entities[english_subject][\"incorrect\"] += 1\n",
    "\n",
    "    # track language\n",
    "    lang = row[1].lang_code\n",
    "\n",
    "    if lang not in entities[english_subject][\"langs\"]:\n",
    "        entities[english_subject][\"langs\"][lang] = 1\n",
    "\n",
    "    else:\n",
    "        entities[english_subject][\"langs\"][lang] += 1\n",
    "\n",
    "    # track any alternate forms\n",
    "    entities[english_subject][\"alternate_forms\"][lang] = subject\n",
    "\n",
    "    entities[english_subject][\"dataset_ids\"].add(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a28da3",
   "metadata": {
    "id": "a9a28da3"
   },
   "outputs": [],
   "source": [
    "entity_names = []\n",
    "correct = []\n",
    "incorrect = []\n",
    "total = []\n",
    "pct = []\n",
    "langs = []\n",
    "num_langs = []\n",
    "alternate_forms = []\n",
    "dataset_ids = []\n",
    "for k, v in entities.items():\n",
    "    entity_names.append(k)\n",
    "    # track # of times entity is used in a correct statement, incorrect, and pct accuracy\n",
    "    correct.append(v[\"correct\"])\n",
    "    incorrect.append(v[\"incorrect\"])\n",
    "    total.append(int(v[\"correct\"]) + int(v[\"incorrect\"]))\n",
    "    pct.append(int(v[\"correct\"]) / (int(v[\"correct\"]) + int(v[\"incorrect\"])))\n",
    "    # track # of languages the entity is used in\n",
    "    langs.append(v[\"langs\"])\n",
    "    num_langs.append(len(v[\"langs\"]))\n",
    "    alternate_forms.append(v[\"alternate_forms\"])\n",
    "    # track dataset ids its used in\n",
    "    dataset_ids.append(list(v[\"dataset_ids\"]))\n",
    "    # sanity check\n",
    "    assert int(v[\"correct\"]) + int(v[\"incorrect\"]) == sum(v[\"langs\"].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e674e",
   "metadata": {
    "id": "c66e674e"
   },
   "outputs": [],
   "source": [
    "# the average entity appears in ~12 langs\n",
    "# (remember that this will max out at 20.)\n",
    "np.mean(num_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef444e",
   "metadata": {
    "id": "a9ef444e"
   },
   "outputs": [],
   "source": [
    "entity_analysis_df = pd.DataFrame(\n",
    "    {\n",
    "        \"entity\": entity_names,\n",
    "        \"num_correct\": correct,\n",
    "        \"num_incorrect\": incorrect,\n",
    "        \"total_usages\": total,\n",
    "        \"percent_accuracy\": pct,\n",
    "        \"languages\": langs,\n",
    "        \"num_languages\": num_langs,\n",
    "        \"alternate_forms\": alternate_forms,\n",
    "        \"dataset_ids\": dataset_ids,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059021c",
   "metadata": {
    "id": "e059021c"
   },
   "source": [
    "## Significance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5284f46c",
   "metadata": {
    "id": "5284f46c"
   },
   "source": [
    "* use chi squared to see: \n",
    " \n",
    "    * if the number of correctly and incorrectly classified statements for each language scrip is statistically significant\n",
    "       * yes\n",
    "    * if the number of correctly and incorrectly classified statements for each language group is statistically significant\n",
    "        * yes\n",
    "        \n",
    "    * the number of correctly and incorrectly classified locations for western/eastern locales is statistically significant\n",
    "    * the number of correctly and incorrectly classified entities for women/men is statistically significant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e2415",
   "metadata": {
    "id": "338e2415"
   },
   "source": [
    "### language groups and script\n",
    "\n",
    "Romance languages: Catalan, French, Italian, Portuguese, Romanian, Spanish\n",
    "Germanic languages: Danish, Dutch, German, Swedish\n",
    "Slavic languages: Bulgarian, Czech, Croatian, Polish, Russian, Serbian, Slovenian, Ukrainian\n",
    "Hungarian: a Uralic language, not related to any of the other languages in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe897b5",
   "metadata": {
    "id": "abe897b5"
   },
   "outputs": [],
   "source": [
    "# 2 x 2\n",
    "lang_to_script_dict = {\n",
    "    \"bg\": \"Cyrillic\",\n",
    "    \"ca\": \"Latin\",\n",
    "    \"cs\": \"Latin\",\n",
    "    \"da\": \"Latin\",\n",
    "    \"de\": \"Latin\",\n",
    "    \"en\": \"Latin\",\n",
    "    \"es\": \"Latin\",\n",
    "    \"fr\": \"Latin\",\n",
    "    \"hr\": \"Latin\",\n",
    "    \"hu\": \"Latin\",\n",
    "    \"it\": \"Latin\",\n",
    "    \"nl\": \"Latin\",\n",
    "    \"pl\": \"Latin\",\n",
    "    \"pt\": \"Latin\",\n",
    "    \"ro\": \"Latin\",\n",
    "    \"ru\": \"Cyrillic\",\n",
    "    \"sl\": \"Latin\",\n",
    "    \"sr\": \"Cyrillic\",\n",
    "    \"sv\": \"Latin\",\n",
    "    \"uk\": \"Cyrillic\",\n",
    "}\n",
    "\n",
    "# 2 x 4\n",
    "lang_to_group_dict = {\n",
    "    \"bg\": \"Slavic\",\n",
    "    \"ca\": \"Romance\",\n",
    "    \"cs\": \"Slavic\",\n",
    "    \"da\": \"Germanic\",\n",
    "    \"de\": \"Germanic\",\n",
    "    \"en\": \"Germanic\",\n",
    "    \"es\": \"Romance\",\n",
    "    \"fr\": \"Romance\",\n",
    "    \"hr\": \"Slavic\",\n",
    "    \"hu\": \"Uralic\",\n",
    "    \"it\": \"Romance\",\n",
    "    \"nl\": \"Germanic\",\n",
    "    \"pl\": \"Slavic\",\n",
    "    \"pt\": \"Romance\",\n",
    "    \"ro\": \"Romance\",\n",
    "    \"ru\": \"Slavic\",\n",
    "    \"sl\": \"Slavic\",\n",
    "    \"sr\": \"Slavic\",\n",
    "    \"sv\": \"Germanic\",\n",
    "    \"uk\": \"Slavic\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f82c1",
   "metadata": {
    "id": "4f0f82c1"
   },
   "outputs": [],
   "source": [
    "# now, for each of these levels, we need:\n",
    "# number correct\n",
    "# number incorrect\n",
    "# total..\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649516b",
   "metadata": {
    "id": "e649516b"
   },
   "outputs": [],
   "source": [
    "# scripts\n",
    "cyrillic = {\"correct\": 0, \"incorrect\": 0}\n",
    "latin = {\"correct\": 0, \"incorrect\": 0}\n",
    "\n",
    "# language groups\n",
    "germanic = {\"correct\": 0, \"incorrect\": 0}\n",
    "romance = {\"correct\": 0, \"incorrect\": 0}\n",
    "slavic = {\"correct\": 0, \"incorrect\": 0}\n",
    "uralic = {\"correct\": 0, \"incorrect\": 0}\n",
    "\n",
    "for row in results_df.iterrows():\n",
    "    lang_code = row[1].lang_code\n",
    "    result = row[1].correct\n",
    "    mapping = \"\"\n",
    "\n",
    "    script = lang_to_script_dict[lang_code]\n",
    "    group = lang_to_group_dict[lang_code]\n",
    "\n",
    "    if result:\n",
    "        mapping = \"correct\"\n",
    "    else:\n",
    "        mapping = \"incorrect\"\n",
    "\n",
    "    # language scripts\n",
    "    if script == \"Cyrillic\":\n",
    "        cyrillic[mapping] += 1\n",
    "    elif script == \"Latin\":\n",
    "        latin[mapping] += 1\n",
    "\n",
    "    # language groups\n",
    "    if group == \"Germanic\":\n",
    "        germanic[mapping] += 1\n",
    "    elif group == \"Romance\":\n",
    "        romance[mapping] += 1\n",
    "    elif group == \"Slavic\":\n",
    "        slavic[mapping] += 1\n",
    "    elif group == \"Uralic\":\n",
    "        uralic[mapping] += 1\n",
    "\n",
    "print(f\"cyrllic: {cyrillic}\")\n",
    "print(f\"latin: {latin}\")\n",
    "\n",
    "print(f\"germanic: {germanic}\")\n",
    "print(f\"romance: {romance}\")\n",
    "print(f\"slavic: {slavic}\")\n",
    "print(\n",
    "    f\"uralic: {uralic}\"\n",
    ")  # sanity check -> this is 75.7% correct which is the same as the llama graph result for HU.\n",
    "\n",
    "# sanity check identical output sizes\n",
    "assert sum(cyrillic.values()) + sum(latin.values()) == results_df.shape[0]\n",
    "assert (\n",
    "    sum(germanic.values())\n",
    "    + sum(romance.values())\n",
    "    + sum(slavic.values())\n",
    "    + sum(uralic.values())\n",
    "    == results_df.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4114f0",
   "metadata": {
    "id": "3c4114f0"
   },
   "outputs": [],
   "source": [
    "def chi_squared(category_dicts, flag_one, flag_two, category_explainer):\n",
    "    table = []\n",
    "    top_vals = []\n",
    "    bottom_vals = []\n",
    "    for i in range(len(category_dicts)):\n",
    "        val = category_dicts[i][flag_one]\n",
    "        top_vals.append(category_dicts[i][flag_one])\n",
    "        bottom_vals.append(category_dicts[i][flag_two])\n",
    "\n",
    "    table = np.array([top_vals, bottom_vals])\n",
    "    # print(f\"contingency_table for {category_explainer}\\n(top row is # correct, bottom row is # incorrect)\\n{table}\")\n",
    "\n",
    "    stat, p, dof, expected = chi2_contingency(table)\n",
    "    reject = \"REJECT\" if p <= 0.05 else \"ACCEPT\"\n",
    "\n",
    "    if p < 0.001:\n",
    "        p = \"< .001\"\n",
    "\n",
    "    print(\n",
    "        f\"For {category_explainer}, we see a chi-squared value of {stat} and a p-value of {p}.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"We can {reject} the null hypothesis that {category_explainer} is independent from performance on the CKA assessment.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb4a1e",
   "metadata": {
    "id": "81fb4a1e"
   },
   "outputs": [],
   "source": [
    "chi_squared([cyrillic, latin], \"correct\", \"incorrect\", \"language script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272561f9",
   "metadata": {
    "id": "272561f9"
   },
   "outputs": [],
   "source": [
    "chi_squared(\n",
    "    [germanic, romance, slavic, uralic], \"correct\", \"incorrect\", \"language family\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559b4da",
   "metadata": {
    "id": "3559b4da"
   },
   "source": [
    "### Western vs Eastern locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f9ac14",
   "metadata": {},
   "source": [
    "### Male vs Female names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f1741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
