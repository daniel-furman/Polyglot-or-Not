{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Attach drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "sOZkGpeZbgxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global var for folder to save results logs to\n",
        "DRIVE_FOLDER_OUT = \"/content/drive/MyDrive/Colab Files/wiki_entity_logs/\""
      ],
      "metadata": {
        "id": "Ky-IzDSGbi1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrpv6q-FX8RD"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/daniel-furman/Capstone.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/Capstone/requirements.txt"
      ],
      "metadata": {
        "id": "9PWDScnWU0aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import json\n",
        "from json import JSONDecodeError\n",
        "import urllib.parse\n",
        "import urllib.request\n",
        "\n",
        "from argparse import Namespace\n",
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "import time\n",
        "import tqdm\n",
        "\n",
        "import re\n",
        "from ftfy import fix_text\n",
        "from string import punctuation\n",
        "\n",
        "import spacy"
      ],
      "metadata": {
        "id": "HtG9zRSyYHY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/Capstone/src/wikipedia_entity_analysis/\")\n",
        "from wiki_analysis import CODE_TO_LANG_DICT, CODE_TO_WIKI_CLEANUP_DICT, CODE_TO_SPACY_MODEL_DICT, load_spacy_models, get_mulitlingual_lookup, get_wikipedia_pages, get_article_info, count_entities_in_article"
      ],
      "metadata": {
        "id": "cd2W4CR-M2uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# args config for grabbing entities\n",
        "args = Namespace(\n",
        "    language=\"ru\",\n",
        "    iterations=1,\n",
        "    articles_per_iter=20,\n",
        "    cleanup_str=CODE_TO_WIKI_CLEANUP_DICT[\"ru\"],\n",
        "    debug=False\n",
        ")"
      ],
      "metadata": {
        "id": "cEkzRStPYKRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_analysis_df = pd.read_csv(\n",
        "    \"../../data/error_analysis/entity_analysis_language_and_accuracy_by_entity.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "vkkZf9BHcmv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get entity <-> multilingual translation lookup resolver\n",
        "target_entities_multiling = get_mulitlingual_lookup(entity_analysis_df, CODE_TO_LANG_DICT)"
      ],
      "metadata": {
        "id": "nBH0mtL7ZJ98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download spacy model\n",
        "model_name = CODE_TO_SPACY_MODEL_DICT[args.language]\n",
        "!python -m spacy download {model_name}"
      ],
      "metadata": {
        "id": "DOMZzFCuZUS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_model = spacy.load(CODE_TO_SPACY_MODEL_DICT[args.language])\n",
        "\n",
        "article_titles = []\n",
        "article_ids = []\n",
        "article_word_counts = []\n",
        "article_full_entities_schedule = []\n",
        "article_full_entities_uniques = []\n",
        "article_full_entities_counts = []\n",
        "article_target_entities_schedule = []\n",
        "article_target_entities_uniques = []\n",
        "article_target_entities_counts = []\n",
        "\n",
        "for i in tqdm.tqdm(range(args.iterations)):\n",
        "\n",
        "    # get pages to parse\n",
        "    pages_to_parse = get_wikipedia_pages(args.language, args.articles_per_iter, args.debug)\n",
        "\n",
        "    # for each page\n",
        "    parsed_pages = 0\n",
        "    for article_id, article_title in tqdm.tqdm(pages_to_parse.items(), position=0, leave=True):\n",
        "        article_data = []\n",
        "        # get info\n",
        "        article_info = get_article_info(article_title, article_id, args.language, args.cleanup_str, args.debug)\n",
        "\n",
        "        # get stats\n",
        "        article_word_count, article_full_entities, article_target_entities = count_entities_in_article(target_entities_multiling, article_info, spacy_model, args.language, args.debug)\n",
        "\n",
        "        # commit stats\n",
        "        article_titles.append(article_title)\n",
        "        article_ids.append(str(article_id))\n",
        "        article_word_counts.append(article_word_count)\n",
        "\n",
        "        if parsed_pages % 50 == 0:\n",
        "            print(f\"\\n====random progress prints====\")\n",
        "            print(f\"retrieved data for {article_title}. {len(article_full_entities)} unique entities and {len(article_target_entities)} unique target entities.\")\n",
        "\n",
        "        # track all entities (vals, total, num unique)\n",
        "        article_full_entities_schedule.append(article_full_entities)\n",
        "        article_full_entities_uniques.append(len(article_full_entities) if bool(article_full_entities) != False else 0)\n",
        "        article_full_entities_counts.append(sum(article_full_entities.values()) if bool(article_full_entities) != False else 0)\n",
        "\n",
        "        # track target entities (vals, total, num unique)\n",
        "        article_target_entities_schedule.append(article_target_entities)\n",
        "        article_target_entities_uniques.append(len(article_target_entities) if bool(article_target_entities) != False else 0)\n",
        "        article_target_entities_counts.append(sum(article_target_entities.values()) if bool(article_target_entities) != False else 0)\n",
        "        \n",
        "        # break for api\n",
        "        time.sleep(.1)\n",
        "\n",
        "        parsed_pages += 1"
      ],
      "metadata": {
        "id": "4ZwcZuqfYryu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log = pd.DataFrame({'article_title': article_titles, 'article_id': article_id, 'article_word_count': article_word_counts,\n",
        "                    'article_full_entities': article_full_entities_schedule, 'article_full_entities_counts': article_full_entities_counts, 'article_full_entities_uniques': article_full_entities_uniques,\n",
        "                    'article_target_entities': article_target_entities_schedule, 'article_target_entities_counts': article_target_entities_counts, 'article_target_entities_uniques': article_target_entities_uniques})\n",
        "log_name = args.language + '-' + str(args.iterations * args.articles_per_iter) + '-' + 'wiki-entity-counts' + '-' + datetime.now().strftime(\"%Y-%m-%d-%Hh-%Mm-%Ss\") + '.json'\n",
        "log_new_path = os.path.join(DRIVE_FOLDER_OUT, log_name)\n",
        "log.to_json(log_new_path, orient=\"index\")"
      ],
      "metadata": {
        "id": "_3mcVg-WhOmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log.head()"
      ],
      "metadata": {
        "id": "tFj3bbaIlxGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(log['article_word_count'])"
      ],
      "metadata": {
        "id": "Uof8J1-FosNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(log['article_full_entities_counts'])"
      ],
      "metadata": {
        "id": "lJ0P9cF0oiFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(log['article_full_entities_uniques'])"
      ],
      "metadata": {
        "id": "b3xcwYscolL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(log['article_target_entities_counts'])"
      ],
      "metadata": {
        "id": "WqKUYx4VomjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(log['article_target_entities_uniques'])"
      ],
      "metadata": {
        "id": "OwU4ndUnooZF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}