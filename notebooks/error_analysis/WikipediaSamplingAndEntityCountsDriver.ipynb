{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Attach drive\n",
    "drive.mount(\"/content/drive\")"
   ],
   "metadata": {
    "id": "sOZkGpeZbgxf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Global var for folder to save results logs to\n",
    "DRIVE_FOLDER_OUT = \"/content/drive/MyDrive/Colab Files/wiki_entity_logs/\""
   ],
   "metadata": {
    "id": "Ky-IzDSGbi1n"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrpv6q-FX8RD"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/daniel-furman/Capstone.git"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -r /content/Capstone/requirements.txt"
   ],
   "metadata": {
    "id": "9PWDScnWU0aY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "from argparse import Namespace\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import re\n",
    "from ftfy import fix_text\n",
    "from string import punctuation\n",
    "\n",
    "import spacy"
   ],
   "metadata": {
    "id": "HtG9zRSyYHY8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "os.chdir(\"/content/Capstone/src/wikipedia_entity_analysis/\")\n",
    "from wiki_analysis import (\n",
    "    CODE_TO_LANG_DICT,\n",
    "    CODE_TO_WIKI_CLEANUP_DICT,\n",
    "    CODE_TO_SPACY_MODEL_DICT,\n",
    "    load_spacy_models,\n",
    "    get_mulitlingual_lookup,\n",
    "    get_wikipedia_pages,\n",
    "    get_article_info,\n",
    "    count_entities_in_article,\n",
    ")"
   ],
   "metadata": {
    "id": "cd2W4CR-M2uv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# args config for grabbing entities\n",
    "args = Namespace(\n",
    "    language=\"ru\",\n",
    "    iterations=1,\n",
    "    articles_per_iter=20,\n",
    "    cleanup_str=CODE_TO_WIKI_CLEANUP_DICT[\"ru\"],\n",
    "    debug=False,\n",
    ")"
   ],
   "metadata": {
    "id": "cEkzRStPYKRV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "entity_analysis_df = pd.read_csv(\n",
    "    \"../../data/error_analysis/entity_analysis_language_and_accuracy_by_entity.csv\"\n",
    ")"
   ],
   "metadata": {
    "id": "vkkZf9BHcmv-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# get entity <-> multilingual translation lookup resolver\n",
    "target_entities_multiling = get_mulitlingual_lookup(\n",
    "    entity_analysis_df, CODE_TO_LANG_DICT\n",
    ")"
   ],
   "metadata": {
    "id": "nBH0mtL7ZJ98"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# download spacy model\n",
    "model_name = CODE_TO_SPACY_MODEL_DICT[args.language]\n",
    "!python -m spacy download {model_name}"
   ],
   "metadata": {
    "id": "DOMZzFCuZUS0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "spacy_model = spacy.load(CODE_TO_SPACY_MODEL_DICT[args.language])\n",
    "\n",
    "article_titles = []\n",
    "article_ids = []\n",
    "article_word_counts = []\n",
    "article_full_entities_schedule = []\n",
    "article_full_entities_uniques = []\n",
    "article_full_entities_counts = []\n",
    "article_target_entities_schedule = []\n",
    "article_target_entities_uniques = []\n",
    "article_target_entities_counts = []\n",
    "\n",
    "for i in tqdm.tqdm(range(args.iterations)):\n",
    "    # get pages to parse\n",
    "    pages_to_parse = get_wikipedia_pages(\n",
    "        args.language, args.articles_per_iter, args.debug\n",
    "    )\n",
    "\n",
    "    # for each page\n",
    "    parsed_pages = 0\n",
    "    for article_id, article_title in tqdm.tqdm(\n",
    "        pages_to_parse.items(), position=0, leave=True\n",
    "    ):\n",
    "        article_data = []\n",
    "        # get info\n",
    "        article_info = get_article_info(\n",
    "            article_title, article_id, args.language, args.cleanup_str, args.debug\n",
    "        )\n",
    "\n",
    "        # get stats\n",
    "        (\n",
    "            article_word_count,\n",
    "            article_full_entities,\n",
    "            article_target_entities,\n",
    "        ) = count_entities_in_article(\n",
    "            target_entities_multiling,\n",
    "            article_info,\n",
    "            spacy_model,\n",
    "            args.language,\n",
    "            args.debug,\n",
    "        )\n",
    "\n",
    "        # commit stats\n",
    "        article_titles.append(article_title)\n",
    "        article_ids.append(str(article_id))\n",
    "        article_word_counts.append(article_word_count)\n",
    "\n",
    "        if parsed_pages % 50 == 0:\n",
    "            print(f\"\\n====random progress prints====\")\n",
    "            print(\n",
    "                f\"retrieved data for {article_title}. {len(article_full_entities)} unique entities and {len(article_target_entities)} unique target entities.\"\n",
    "            )\n",
    "\n",
    "        # track all entities (vals, total, num unique)\n",
    "        article_full_entities_schedule.append(article_full_entities)\n",
    "        article_full_entities_uniques.append(\n",
    "            len(article_full_entities) if bool(article_full_entities) != False else 0\n",
    "        )\n",
    "        article_full_entities_counts.append(\n",
    "            sum(article_full_entities.values())\n",
    "            if bool(article_full_entities) != False\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # track target entities (vals, total, num unique)\n",
    "        article_target_entities_schedule.append(article_target_entities)\n",
    "        article_target_entities_uniques.append(\n",
    "            len(article_target_entities)\n",
    "            if bool(article_target_entities) != False\n",
    "            else 0\n",
    "        )\n",
    "        article_target_entities_counts.append(\n",
    "            sum(article_target_entities.values())\n",
    "            if bool(article_target_entities) != False\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # break for api\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        parsed_pages += 1"
   ],
   "metadata": {
    "id": "4ZwcZuqfYryu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "log = pd.DataFrame(\n",
    "    {\n",
    "        \"article_title\": article_titles,\n",
    "        \"article_id\": article_id,\n",
    "        \"article_word_count\": article_word_counts,\n",
    "        \"article_full_entities\": article_full_entities_schedule,\n",
    "        \"article_full_entities_counts\": article_full_entities_counts,\n",
    "        \"article_full_entities_uniques\": article_full_entities_uniques,\n",
    "        \"article_target_entities\": article_target_entities_schedule,\n",
    "        \"article_target_entities_counts\": article_target_entities_counts,\n",
    "        \"article_target_entities_uniques\": article_target_entities_uniques,\n",
    "    }\n",
    ")\n",
    "log_name = (\n",
    "    args.language\n",
    "    + \"-\"\n",
    "    + str(args.iterations * args.articles_per_iter)\n",
    "    + \"-\"\n",
    "    + \"wiki-entity-counts\"\n",
    "    + \"-\"\n",
    "    + datetime.now().strftime(\"%Y-%m-%d-%Hh-%Mm-%Ss\")\n",
    "    + \".json\"\n",
    ")\n",
    "log_new_path = os.path.join(DRIVE_FOLDER_OUT, log_name)\n",
    "log.to_json(log_new_path, orient=\"index\")"
   ],
   "metadata": {
    "id": "_3mcVg-WhOmC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "log.head()"
   ],
   "metadata": {
    "id": "tFj3bbaIlxGt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.mean(log[\"article_word_count\"])"
   ],
   "metadata": {
    "id": "Uof8J1-FosNs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.mean(log[\"article_full_entities_counts\"])"
   ],
   "metadata": {
    "id": "lJ0P9cF0oiFn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.mean(log[\"article_full_entities_uniques\"])"
   ],
   "metadata": {
    "id": "b3xcwYscolL0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.mean(log[\"article_target_entities_counts\"])"
   ],
   "metadata": {
    "id": "WqKUYx4VomjI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.mean(log[\"article_target_entities_uniques\"])"
   ],
   "metadata": {
    "id": "OwU4ndUnooZF"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}