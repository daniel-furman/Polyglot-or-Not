{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Attach drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "sOZkGpeZbgxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global var for folder to save results logs to\n",
        "DRIVE_FOLDER_OUT = \"/content/drive/MyDrive/Colab Files/wiki_entity_logs/\""
      ],
      "metadata": {
        "id": "Ky-IzDSGbi1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrpv6q-FX8RD"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/daniel-furman/Capstone.git\n",
        "!pip install -r /content/Capstone/requirements.txt\n",
        "#!pip install -r /content/Capstone/requirements_llama.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import json\n",
        "from json import JSONDecodeError\n",
        "import urllib.parse\n",
        "import urllib.request\n",
        "\n",
        "from argparse import Namespace\n",
        "\n",
        "import os\n",
        "\n",
        "from datetime import date\n",
        "\n",
        "import re\n",
        "from ftfy import fix_text\n",
        "from string import punctuation\n",
        "\n",
        "import spacy\n",
        "\n",
        "os.chdir(\"/content/Capstone/src/wikipedia_entity_analysis/\"\n",
        "from wiki_analysis import CODE_TO_LANG_DICT, CODE_TO_WIKI_CLEANUP_DICT, CODE_TO_SPACY_MODEL_DICT, load_spacy_models, get_mulitlingual_lookup, get_wikipedia_pages, get_article_info, count_entities_in_article, \n"
      ],
      "metadata": {
        "id": "HtG9zRSyYHY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_analysis_df = pd.read_csv(\n",
        "    \"../../data/error_analysis/entity_analysis_language_and_accuracy_by_entity.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "xb4BPLmPYIVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# args config for grabbing entities\n",
        "args = Namespace(\n",
        "    language=\"en\",\n",
        "    iterations=100,\n",
        "    cleanup_str=CODE_TO_WIKI_CLEANUP_DICT[\"en\"],\n",
        "    debug=False\n",
        ")"
      ],
      "metadata": {
        "id": "cEkzRStPYKRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get entity <-> multilingual translation lookup resolver\n",
        "target_entities_multiling = get_mulitlingual_lookup(get_mulitlingual_lookup)"
      ],
      "metadata": {
        "id": "nBH0mtL7ZJ98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load spacy model\n",
        "model_name = CODE_TO_SPACY_MODEL_DICT[args.language]\n",
        "!python -m spacy download {model_name}"
      ],
      "metadata": {
        "id": "DOMZzFCuZUS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_model = load_spacy_models(CODE_TO_SPACY_MODEL_DICT[args.language])\n",
        "\n",
        "article_titles = []\n",
        "article_ids = []\n",
        "article_word_counts = []\n",
        "article_full_entities = []\n",
        "article_target_entities = []\n",
        "article_summary_strs = []\n",
        "for i in range(args.iterations):\n",
        "    # get pages to parse\n",
        "    pages_to_parse = get_wikipedia_pages(args.language, args.debug)\n",
        "\n",
        "    # for each page\n",
        "    for page in pages_to_parse:\n",
        "        article_data = []\n",
        "        article_id = list(page.keys())[0]\n",
        "        article_title = list(page.items())[0]\n",
        "        # get info\n",
        "        article_info = get_article_info(article_title, article_id, args.language, cleanup_str, args.debug)\n",
        "        # get stats\n",
        "        article_word_count, article_full_entities, article_target_entities = count_entities_in_article(target_entities_multiling, article_info, spacy_model, args.language, debug)\n",
        "        article_summary_str = f\"article titled {article_title} with pageid {article_id} has {article_word_count} words, {len(article_full_entities)} unique entity mentions {sum(article_full_entities.values())} total entity mentions, {len(article_target_entities)} unique target entities and {sum(article_target_entities.values())} total target entity mentions\"\n",
        "        \n",
        "        if args.debug:\n",
        "            print(article_summary_str)\n",
        "        \n",
        "        # commit stats\n",
        "        article_titles.append(article_title)\n",
        "        article_ids.append(article_id)\n",
        "        article_word_counts.append(article_word_count)\n",
        "        article_full_entities.append(article_full_entities)\n",
        "        article_target_entities.append(article_target_entities)\n",
        "        article_summary_strs.append(article_summary_str)\n",
        "        break\n",
        "    break\n",
        "\n",
        "log = pd.DataFrame({'article_title': article_titles, 'article_id': article_id, 'article_word_count': article_word_counts, 'article_full_entities': article_full_entites, 'article_target_entities': article_target_entities, 'article_summary': article_summary_strs})\n",
        "log_name = args.language + '-' + args.iterations + 'wikipedia-entity-count' + str(date.today())\n",
        "log_new_path = os.path.join(DRIVE_FOLDER_OUT, log_name)\n",
        "log.to_csv(log_new_path, encoding='utf-8', index=False))"
      ],
      "metadata": {
        "id": "4ZwcZuqfYryu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save result logs to drive\n",
        "\n",
        "log = glob.glob(f\"/content/Capstone/src/fact_completion_scripts/{log_fpath}\")[0]\n",
        "log_name = args.language + '-' + log.split(\"/\")[-1]\n",
        "log_new_path = os.path.join(DRIVE_FOLDER_OUT, log_name)\n",
        "!cp {log} '{log_new_path}'"
      ],
      "metadata": {
        "id": "3aPOQrCobnef"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}