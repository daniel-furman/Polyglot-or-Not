{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fb5fbc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484138e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "from ftfy import fix_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ecc316",
   "metadata": {},
   "source": [
    "## Load Entity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20aaef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_lang_dict = {\n",
    "    \"bg\": \"Bulgarian\",\n",
    "    \"ca\": \"Catalan\",\n",
    "    \"cs\": \"Czech\",\n",
    "    \"da\": \"Danish\",\n",
    "    \"de\": \"German\",\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"fr\": \"French\",\n",
    "    \"hr\": \"Croatian\",\n",
    "    \"hu\": \"Hungarian\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"nl\": \"Dutch\",\n",
    "    \"pl\": \"Polish\",\n",
    "    \"pt\": \"Portuguese\",\n",
    "    \"ro\": \"Romanian\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"sl\": \"Slovenian\",\n",
    "    \"sr\": \"Serbian\",\n",
    "    \"sv\": \"Swedish\",\n",
    "    \"uk\": \"Ukrainian\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ea8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_spacy_model_dict = {\n",
    "    \"ca\": \"ca_core_news_lg\",\n",
    "    \"da\": \"da_core_news_lg\",\n",
    "    \"de\": \"de_core_news_lg\",\n",
    "    \"en\": \"en_core_news_lg\",\n",
    "    \"es\": \"es_core_news_lg\",\n",
    "    \"fr\": \"fr_core_news_lg\",\n",
    "    \"hr\": \"hr_core_news_lg\",\n",
    "    \"it\": \"it_core_news_lg\",\n",
    "    \"nl\": \"nl_core_news_lg\",\n",
    "    \"pl\": \"pl_core_news_lg\",\n",
    "    \"pt\": \"pt_core_news_lg\",\n",
    "    \"ro\": \"ro_core_news_lg\",\n",
    "    \"ru\": \"ru_core_news_lg\",\n",
    "    \"sv\": \"sv_core_news_lg\",\n",
    "    \"uk\": \"uk_core_news_lg\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d350b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_codes = list(code_to_lang_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c48dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_analysis_df = pd.read_csv(\n",
    "    \"../../data/error_analysis/entity_analysis_language_and_accuracy_by_entity.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f76ba1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>num_correct</th>\n",
       "      <th>num_incorrect</th>\n",
       "      <th>total_usages</th>\n",
       "      <th>percent_accuracy</th>\n",
       "      <th>languages</th>\n",
       "      <th>num_languages</th>\n",
       "      <th>alternate_forms</th>\n",
       "      <th>dataset_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prius</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'sr': 1, 'uk': 1, 'nl': 1, 'sv': 1, 'ca': 1, ...</td>\n",
       "      <td>16</td>\n",
       "      <td>{'sr': 'Приус', 'uk': 'Prius', 'nl': 'Prius', ...</td>\n",
       "      <td>['calinet_8922']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sundar Pichai</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>{'sr': 1, 'uk': 1, 'nl': 1, 'sv': 1, 'ca': 1, ...</td>\n",
       "      <td>19</td>\n",
       "      <td>{'sr': 'Сундар Пицхаи', 'uk': 'Сундар Пічаї', ...</td>\n",
       "      <td>['rome_5025']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>People's Republic of China</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'sr': 1, 'uk': 1, 'nl': 1, 'sv': 1, 'hu': 1, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>{'sr': 'Народна Република Кина', 'uk': 'Народн...</td>\n",
       "      <td>['rome_21333']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sint Maarten</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>{'sr': 1, 'nl': 2, 'sv': 1, 'ca': 1, 'pl': 1, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>{'sr': 'Синт Маартен', 'nl': 'Sint Maarten', '...</td>\n",
       "      <td>['rome_8738', 'rome_20596']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Haas House</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>{'sr': 1, 'nl': 1, 'sv': 1, 'hu': 1, 'ca': 1, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>{'sr': 'Хаас Хоусе', 'nl': 'Haas House', 'sv':...</td>\n",
       "      <td>['rome_8783']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       entity  num_correct  num_incorrect  total_usages  \\\n",
       "0                       Prius           16              0            16   \n",
       "1               Sundar Pichai           18              1            19   \n",
       "2  People's Republic of China           17              0            17   \n",
       "3                Sint Maarten           11             10            21   \n",
       "4                  Haas House            9              5            14   \n",
       "\n",
       "   percent_accuracy                                          languages  \\\n",
       "0          1.000000  {'sr': 1, 'uk': 1, 'nl': 1, 'sv': 1, 'ca': 1, ...   \n",
       "1          0.947368  {'sr': 1, 'uk': 1, 'nl': 1, 'sv': 1, 'ca': 1, ...   \n",
       "2          1.000000  {'sr': 1, 'uk': 1, 'nl': 1, 'sv': 1, 'hu': 1, ...   \n",
       "3          0.523810  {'sr': 1, 'nl': 2, 'sv': 1, 'ca': 1, 'pl': 1, ...   \n",
       "4          0.642857  {'sr': 1, 'nl': 1, 'sv': 1, 'hu': 1, 'ca': 1, ...   \n",
       "\n",
       "   num_languages                                    alternate_forms  \\\n",
       "0             16  {'sr': 'Приус', 'uk': 'Prius', 'nl': 'Prius', ...   \n",
       "1             19  {'sr': 'Сундар Пицхаи', 'uk': 'Сундар Пічаї', ...   \n",
       "2             17  {'sr': 'Народна Република Кина', 'uk': 'Народн...   \n",
       "3             14  {'sr': 'Синт Маартен', 'nl': 'Sint Maarten', '...   \n",
       "4             14  {'sr': 'Хаас Хоусе', 'nl': 'Haas House', 'sv':...   \n",
       "\n",
       "                   dataset_ids  \n",
       "0             ['calinet_8922']  \n",
       "1                ['rome_5025']  \n",
       "2               ['rome_21333']  \n",
       "3  ['rome_8738', 'rome_20596']  \n",
       "4                ['rome_8783']  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_analysis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7ea8c",
   "metadata": {},
   "source": [
    "So we have 23k entities to work with. We're interested in how many times they get mentioned on wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7efb860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_analysis_df['entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a85fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given language, randomly sample <n> articles (max of 500).\n",
    "# return a dict of their id and title.\n",
    "def get_wikipedia_pages(lang, debug = False):\n",
    "    \n",
    "    # construct URL for API call\n",
    "    articles_url = f\"https://{lang}.wikipedia.org/w/api.php?action=query&list=random&format=json&rnnamespace=0&rnlimit=50&format=json\"\n",
    "    \n",
    "    # grab data\n",
    "    url = urllib.request.urlopen(articles_url)\n",
    "    \n",
    "    # read data\n",
    "    data = url.read()\n",
    "    \n",
    "    # set encoding and load into obj\n",
    "    encoding = url.info().get_content_charset('utf-8')\n",
    "    obj = json.loads(data.decode(encoding))\n",
    "    \n",
    "    if 'query' not in obj or 'random' not in obj['query']:\n",
    "        if debug:\n",
    "            print(f\"Unable to grab articles from {code_to_lang_dict[lang]} using URL {url}.\")\n",
    "        raise Exception\n",
    "        \n",
    "    mappings = obj['query']['random']\n",
    "    ids = {}\n",
    "    for m in mappings:\n",
    "        ids[m['id']] = m['title']\n",
    "        \n",
    "    if debug:\n",
    "        print(f\"Fetched {len(ids)} articles from {code_to_lang_dict[lang]} wikipedia\")\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04273ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 50 articles from Bulgarian wikipedia\n"
     ]
    }
   ],
   "source": [
    "info_to_check = get_wikipedia_pages(lang_codes[0], debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e4fa9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{515511: 'Hippocampus patagonicus',\n",
       " 782753: 'Ден на енергетика',\n",
       " 155589: 'Малки Зондски острови',\n",
       " 278662: 'Градац',\n",
       " 553496: 'Acropora bushyensis',\n",
       " 658845: 'God Hates Us All',\n",
       " 298323: 'Марк Юний Метий Руф',\n",
       " 155968: 'Геополитика',\n",
       " 438116: 'Даяна Апълярд',\n",
       " 514282: 'Rhipidomys venustus',\n",
       " 403001: 'Добромир Добрев',\n",
       " 656236: 'Генчо Скордев',\n",
       " 509713: 'Ficedula hyperythra',\n",
       " 89001: 'Лази',\n",
       " 122222: 'Териер',\n",
       " 219032: 'Пафта',\n",
       " 47846: 'Траурни потапници',\n",
       " 207079: 'Инвърклайд',\n",
       " 297410: 'Публий Стертиний Кварт',\n",
       " 567237: 'Holothuria impatiens',\n",
       " 82135: 'Кюдо',\n",
       " 244488: 'Иван Георгиев (агроном)',\n",
       " 782117: 'Джилиан Филип',\n",
       " 467918: 'Извън обхват',\n",
       " 517836: 'Cookeconcha contorta',\n",
       " 675501: 'Август Лудвиг фон Барби-Мюлинген',\n",
       " 633750: 'Георги Паспалев',\n",
       " 430188: 'Вилейка',\n",
       " 826479: 'Москитос',\n",
       " 122020: 'Лейкуд (Калифорния)',\n",
       " 118986: 'Венцислав Върбанов',\n",
       " 649225: 'Филип Аврамов (композитор)',\n",
       " 123828: 'Евмел',\n",
       " 643241: 'Heinkel He 176',\n",
       " 663471: 'Митрофан III Константинополски',\n",
       " 35272: 'Протагор',\n",
       " 290467: 'Пенка Цицелкова',\n",
       " 556276: 'Bagrus bajad',\n",
       " 563506: 'Ивелина Василева',\n",
       " 561306: 'Cambaroides dauricus',\n",
       " 732692: 'А дано, ама надали',\n",
       " 496020: '133 (число)',\n",
       " 426014: 'Иво Иванов (химик)',\n",
       " 72026: 'Георги Димитров (социолог)',\n",
       " 425909: 'Ангелът унищожител',\n",
       " 161217: 'Микроядро',\n",
       " 470668: 'Елизабета Баварска (1383-1442)',\n",
       " 614790: 'ФК Бенковски 2006',\n",
       " 169301: 'Кадилак',\n",
       " 313196: 'Улпия (баба на Адриан)'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "719d9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for an inputted article_id:title combination\n",
    "we want to hit:\n",
    "https://en.wikipedia.org/w/api.php?action=query&format=json&titles=Kerala&prop=extracts&explaintext\n",
    "good response - {\"batchcomplete\":\"\",\"query\":{\"pages\":{\"14958\":{\"pageid\":14958,\"ns\":0,\"title\":\"Kerala\"\n",
    "bad response  - {\"batchcomplete\":\"\",\"query\":{\"pages\":{\"-1\":{\"ns\":0,\"title\":\"Kerala\",\"missing\":\"\"}}}}\n",
    "'''\n",
    "\n",
    "def get_article_info(article_title, pageid, lang, debug=False):\n",
    "    \n",
    "    # val\n",
    "    if article_title == '' or article_title is None:\n",
    "        if debug:\n",
    "            print(\"Can't parse empty title.\")\n",
    "        return {}\n",
    "    \n",
    "    if lang  == '' or lang is None:\n",
    "        if debug:\n",
    "            print(\"Input a language.\")\n",
    "        return {}\n",
    "    \n",
    "    lang = lang.lower()\n",
    "      \n",
    "    url = \"\"\n",
    "    \n",
    "    # format title via quote escapes to ensure non-ascii chars can get handed off properly\n",
    "    quoted_title = urllib.parse.quote(article_title)\n",
    "    \n",
    "    # construct url\n",
    "    # where lang is the language we are requested\n",
    "    # and quoted title refers to our article\n",
    "    info_url = f\"https://{lang}.wikipedia.org/w/api.php?action=query&format=json&titles={quoted_title}&prop=extracts&explaintext&format=json\"\n",
    "        \n",
    "    \n",
    "    if debug:\n",
    "        print(f\"calling {info_url} to retrieve info about {article_title} from {lang} wiki.\")\n",
    "    \n",
    "    # grab data\n",
    "    try:\n",
    "        url = urllib.request.urlopen(info_url)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"could not decode API call for {article_title} on {lang} wiki; url is {info_url}.\")\n",
    "        return {}\n",
    "        \n",
    "    # read content\n",
    "    data = url.read()\n",
    "        \n",
    "    # set encoding and load into obj\n",
    "    encoding = url.info().get_content_charset('utf-8')\n",
    "    obj = json.loads(data.decode(encoding))\n",
    "    \n",
    "    if 'query' not in obj or 'pages' not in obj['query']:\n",
    "        if debug:\n",
    "            print(f\"Error parsing response for {article_title} from {lang} wiki.\")\n",
    "        raise Exception\n",
    "    \n",
    "    # check for a 'missing'/bad response\n",
    "    if -1 in obj['query']['pages'].keys():\n",
    "        if debug:\n",
    "            print(f\"No wiki data found for {article_title} on {lang} wiki.\")\n",
    "        return {}\n",
    "     \n",
    "    # get pageid of the returned article\n",
    "    data_pageid = list(obj['query']['pages'].keys())[0]\n",
    "    \n",
    "    # double check pageid matches the one returned by API\n",
    "    if data_pageid != pageid:\n",
    "        if debug:\n",
    "            print(f\"id mismatch -- excpected {pageid} but retrieved {data_pageid} for {article_title} on {lang} wiki.\")\n",
    "        return {}\n",
    "    \n",
    "    # check if text is properly returned\n",
    "    if 'extract' not in obj['query']['pages'][data_pageid]:\n",
    "        if debug:\n",
    "            print(f\"could not retrieve text from {pageid} {article_title} on {lang} wiki.\")\n",
    "        return {}\n",
    "    \n",
    "    # get text\n",
    "    content = obj['query']['pages'][data_pageid]['extract']\n",
    "    \n",
    "    # fix text\n",
    "    content = fix_text(content)\n",
    "        \n",
    "    return {article_title: content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1588b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_article_info('Кун Теменужков','645695', 'bg', debug=True)\n",
    "obama_info = get_article_info('Barack Obama','534366', 'en', debug=False)\n",
    "# get_article_info('Penguin Books', '925064', 'fr', debug=True)\n",
    "# get_article_info('Barack Obama','430434', 'es', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b513519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy_models(langs_to_use, code_to_spacy_model_dict):\n",
    "    container = []\n",
    "    for lang in langs_to_use:\n",
    "        container.append({['lang']: spacy.load(code_to_spacy_model_dict[lang])})\n",
    "    \n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4178e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for inputted article content\n",
    "# count how many of our target entities appear in the text\n",
    "# always search english and the native language in-case of translation inconsistencies\n",
    "def count_entities_in_article(target_entity, article_content, language, debug=False):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59dcede6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nnlp = spacy.load(...) # load your model\\n\\nfrom collections import Counter\\n\\nents = Counter()\\n\\ntext = ... # your text\\nfor ent in nlp(text).ents:\\n    ents[f\"{ent.label_}:{ent.text}\"] += 1\\n\\nfor key, val in ents.items():\\n    print(val, key, sep=\"\\t\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "nlp = spacy.load(...) # load your model\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "ents = Counter()\n",
    "\n",
    "text = ... # your text\n",
    "for ent in nlp(text).ents:\n",
    "    ents[f\"{ent.label_}:{ent.text}\"] += 1\n",
    "\n",
    "for key, val in ents.items():\n",
    "    print(val, key, sep=\"\\t\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d522b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
