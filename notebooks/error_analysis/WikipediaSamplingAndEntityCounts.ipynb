{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93fb5fbc",
      "metadata": {
        "id": "93fb5fbc"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/daniel-furman/Capstone.git\n",
        "!pip install -r /content/Capstone/requirements.txt\n",
        "#!pip install -r /content/Capstone/requirements_llama.txt"
      ],
      "metadata": {
        "id": "kFVvVvCIoNpj"
      },
      "id": "kFVvVvCIoNpj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484138e4",
      "metadata": {
        "id": "484138e4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import json\n",
        "from json import JSONDecodeError\n",
        "import urllib.parse\n",
        "import urllib.request\n",
        "\n",
        "import os\n",
        "\n",
        "import re\n",
        "from ftfy import fix_text\n",
        "from string import punctuation\n",
        "\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/Capstone/notebooks/error_analysis\")"
      ],
      "metadata": {
        "id": "yUskpRVupyes"
      },
      "id": "yUskpRVupyes",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d8ecc316",
      "metadata": {
        "id": "d8ecc316"
      },
      "source": [
        "## Load Entity Data and Spacy models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20aaef30",
      "metadata": {
        "id": "20aaef30"
      },
      "outputs": [],
      "source": [
        "code_to_lang_dict = {\n",
        "    \"bg\": \"Bulgarian\",\n",
        "    \"ca\": \"Catalan\",\n",
        "    \"cs\": \"Czech\",\n",
        "    \"da\": \"Danish\",\n",
        "    \"de\": \"German\",\n",
        "    \"en\": \"English\",\n",
        "    \"es\": \"Spanish\",\n",
        "    \"fr\": \"French\",\n",
        "    \"hr\": \"Croatian\",\n",
        "    \"hu\": \"Hungarian\",\n",
        "    \"it\": \"Italian\",\n",
        "    \"nl\": \"Dutch\",\n",
        "    \"pl\": \"Polish\",\n",
        "    \"pt\": \"Portuguese\",\n",
        "    \"ro\": \"Romanian\",\n",
        "    \"ru\": \"Russian\",\n",
        "    \"sl\": \"Slovenian\",\n",
        "    \"sr\": \"Serbian\",\n",
        "    \"sv\": \"Swedish\",\n",
        "    \"uk\": \"Ukrainian\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_to_wiki_cleanup_dict = {\n",
        "    \"ca\": \"Referèncie\",\n",
        "    \"da\": \"Litteratur\",\n",
        "    \"de\": \"Literatur\",\n",
        "    \"en\": \"References\",\n",
        "    \"es\": \"Referencias\",\n",
        "    \"fr\": \"Notes et références\",\n",
        "    \"hr\": \"Izvori\",\n",
        "    \"it\": \"Note\",\n",
        "    \"nl\": \"Literatuur\",\n",
        "    \"pl\": \"Przypisy\",\n",
        "    \"pt\": \"Referências\",\n",
        "    \"ro\": \"Note\",\n",
        "    \"ru\": \"Примечания\",\n",
        "    \"sv\": \"Källor\",\n",
        "    \"uk\": \"Література\",\n",
        "}"
      ],
      "metadata": {
        "id": "j9q3v7wkytZs"
      },
      "id": "j9q3v7wkytZs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ea8ff8",
      "metadata": {
        "id": "29ea8ff8"
      },
      "outputs": [],
      "source": [
        "# one could change this to a different model like their\n",
        "# transformer based variant\n",
        "# but that is not available for every language we want to work with\n",
        "code_to_spacy_model_dict = {\n",
        "    \"ca\": \"ca_core_news_lg\",\n",
        "    \"da\": \"da_core_news_lg\",\n",
        "    \"de\": \"de_core_news_lg\",\n",
        "    \"en\": \"en_core_web_lg\",\n",
        "    \"es\": \"es_core_news_lg\",\n",
        "    \"fr\": \"fr_core_news_lg\",\n",
        "    \"hr\": \"hr_core_news_lg\",\n",
        "    \"it\": \"it_core_news_lg\",\n",
        "    \"nl\": \"nl_core_news_lg\",\n",
        "    \"pl\": \"pl_core_news_lg\",\n",
        "    \"pt\": \"pt_core_news_lg\",\n",
        "    \"ro\": \"ro_core_news_lg\",\n",
        "    \"ru\": \"ru_core_news_lg\",\n",
        "    \"sv\": \"sv_core_news_lg\",\n",
        "    \"uk\": \"uk_core_news_lg\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download {code_to_spacy_model_dict[\"ca\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"da\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"de\"]}\n",
        "!python -m spacy download {code_to_spacy_model_dict[\"en\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"es\"]}\n",
        "!python -m spacy download {code_to_spacy_model_dict[\"fr\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"hr\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"it\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"nl\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"pl\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"pt\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"ro\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"ru\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"sv\"]}\n",
        "# !python -m spacy download {code_to_spacy_model_dict[\"uk\"]}"
      ],
      "metadata": {
        "id": "4A9KCKqDqrdo"
      },
      "id": "4A9KCKqDqrdo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b513519f",
      "metadata": {
        "id": "b513519f"
      },
      "outputs": [],
      "source": [
        "def load_spacy_models(code_to_spacy_model_dict):\n",
        "    container = {}\n",
        "    for lang, model in code_to_spacy_model_dict.items():\n",
        "        container[lang] = spacy.load(model)\n",
        "\n",
        "    return container"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_models = load_spacy_models(code_to_spacy_model_dict)"
      ],
      "metadata": {
        "id": "GXS9v6DUqJTP"
      },
      "id": "GXS9v6DUqJTP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d350b9af",
      "metadata": {
        "id": "d350b9af"
      },
      "outputs": [],
      "source": [
        "lang_codes = list(code_to_lang_dict.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c48dd74",
      "metadata": {
        "id": "0c48dd74"
      },
      "outputs": [],
      "source": [
        "entity_analysis_df = pd.read_csv(\n",
        "    \"../../data/error_analysis/entity_analysis_language_and_accuracy_by_entity.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76ba1c9",
      "metadata": {
        "id": "f76ba1c9"
      },
      "outputs": [],
      "source": [
        "entity_analysis_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acf7ea8c",
      "metadata": {
        "id": "acf7ea8c"
      },
      "source": [
        "So we have 23k entities to work with. We're interested in how many times they get mentioned on wikipedia."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Entities and Their Translated Forms"
      ],
      "metadata": {
        "id": "jkkdr46hXEEQ"
      },
      "id": "jkkdr46hXEEQ"
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(entity_analysis_df[\"entity\"]))"
      ],
      "metadata": {
        "id": "U6dLYfqo5hTg"
      },
      "id": "U6dLYfqo5hTg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7efb860",
      "metadata": {
        "id": "f7efb860"
      },
      "outputs": [],
      "source": [
        "# get lookup that connects the english form of an entity to its multilingual version\n",
        "# annoying that with the way the DF is set up right now, have to do manual cleanup to extract the translated forms\n",
        "# should update the other NB so that it ouptuts a well formatted json into the column\n",
        "target_entities_multiling = {}\n",
        "for row in entity_analysis_df.iterrows():\n",
        "    d = row[1].alternate_forms\n",
        "    for code in code_to_lang_dict.keys():\n",
        "        d = d.replace(\"'\" + code + \"'\", '\"' + code + '\"')\n",
        "\n",
        "    d = d.replace(\": '\", ': \"')\n",
        "    d = d.replace(\"',\", '\",')\n",
        "    d = d.replace(\"'}\", '\"}')\n",
        "    d = d.replace('\"\"', '\"')\n",
        "    try:\n",
        "        d = json.loads(d)\n",
        "        target_entities_multiling[row[1].entity] = d\n",
        "\n",
        "    except JSONDecodeError:\n",
        "        print(f\"couldn't parse {d}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_entities_multiling)"
      ],
      "metadata": {
        "id": "LR-Uz8hA4uIb"
      },
      "id": "LR-Uz8hA4uIb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data From Wikipedia"
      ],
      "metadata": {
        "id": "a8RlfQo0XIKk"
      },
      "id": "a8RlfQo0XIKk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a85fa95",
      "metadata": {
        "id": "7a85fa95"
      },
      "outputs": [],
      "source": [
        "# for a given language, randomly sample <n> articles (max of 500).\n",
        "# return a dict of their id and title.\n",
        "def get_wikipedia_pages(lang, debug=False):\n",
        "    # construct URL for API call\n",
        "    articles_url = f\"https://{lang}.wikipedia.org/w/api.php?action=query&list=random&format=json&rnnamespace=0&rnlimit=50&format=json\"\n",
        "\n",
        "    # grab data\n",
        "    url = urllib.request.urlopen(articles_url)\n",
        "\n",
        "    # read data\n",
        "    data = url.read()\n",
        "\n",
        "    # set encoding and load into obj\n",
        "    encoding = url.info().get_content_charset(\"utf-8\")\n",
        "    obj = json.loads(data.decode(encoding))\n",
        "\n",
        "    if \"query\" not in obj or \"random\" not in obj[\"query\"]:\n",
        "        if debug:\n",
        "            print(\n",
        "                f\"Unable to grab articles from {code_to_lang_dict[lang]} using URL {url}.\"\n",
        "            )\n",
        "        raise Exception\n",
        "\n",
        "    mappings = obj[\"query\"][\"random\"]\n",
        "    ids = {}\n",
        "    for m in mappings:\n",
        "        ids[m[\"id\"]] = m[\"title\"]\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Fetched {len(ids)} articles from {code_to_lang_dict[lang]} wikipedia\")\n",
        "    return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "719d9d98",
      "metadata": {
        "id": "719d9d98"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "for an inputted article_id:title combination\n",
        "we want to hit:\n",
        "https://en.wikipedia.org/w/api.php?action=query&format=json&titles=Kerala&prop=extracts&explaintext\n",
        "good response - {\"batchcomplete\":\"\",\"query\":{\"pages\":{\"14958\":{\"pageid\":14958,\"ns\":0,\"title\":\"Kerala\"\n",
        "bad response  - {\"batchcomplete\":\"\",\"query\":{\"pages\":{\"-1\":{\"ns\":0,\"title\":\"Kerala\",\"missing\":\"\"}}}}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_article_info(\n",
        "    article_title, pageid, lang, code_to_wiki_cleanup_dict, debug=False\n",
        "):\n",
        "    # val\n",
        "    if article_title == \"\" or article_title is None:\n",
        "        if debug:\n",
        "            print(\"Can't parse empty title.\")\n",
        "        return {}\n",
        "\n",
        "    if lang == \"\" or lang is None:\n",
        "        if debug:\n",
        "            print(\"Input a language.\")\n",
        "        return {}\n",
        "\n",
        "    lang = lang.lower()\n",
        "\n",
        "    url = \"\"\n",
        "\n",
        "    # format title via quote escapes to ensure non-ascii chars can get handed off properly\n",
        "    quoted_title = urllib.parse.quote(article_title)\n",
        "\n",
        "    # construct url\n",
        "    # where lang is the language we are requested\n",
        "    # and quoted title refers to our article\n",
        "    info_url = f\"https://{lang}.wikipedia.org/w/api.php?action=query&format=json&titles={quoted_title}&prop=extracts&explaintext&format=json\"\n",
        "\n",
        "    if debug:\n",
        "        print(\n",
        "            f\"calling {info_url} to retrieve info about {article_title} from {lang} wiki.\"\n",
        "        )\n",
        "\n",
        "    # grab data\n",
        "    try:\n",
        "        url = urllib.request.urlopen(info_url)\n",
        "    except UnicodeDecodeError:\n",
        "        print(\n",
        "            f\"could not decode API call for {article_title} on {lang} wiki; url is {info_url}.\"\n",
        "        )\n",
        "        return {}\n",
        "\n",
        "    # read content\n",
        "    data = url.read()\n",
        "\n",
        "    # set encoding and load into obj\n",
        "    encoding = url.info().get_content_charset(\"utf-8\")\n",
        "    obj = json.loads(data.decode(encoding))\n",
        "\n",
        "    if \"query\" not in obj or \"pages\" not in obj[\"query\"]:\n",
        "        if debug:\n",
        "            print(f\"Error parsing response for {article_title} from {lang} wiki.\")\n",
        "        raise Exception\n",
        "\n",
        "    # check for a 'missing'/bad response\n",
        "    if -1 in obj[\"query\"][\"pages\"].keys():\n",
        "        if debug:\n",
        "            print(f\"No wiki data found for {article_title} on {lang} wiki.\")\n",
        "        return {}\n",
        "\n",
        "    # get pageid of the returned article\n",
        "    data_pageid = list(obj[\"query\"][\"pages\"].keys())[0]\n",
        "\n",
        "    # double check pageid matches the one returned by API\n",
        "    if data_pageid != pageid:\n",
        "        if debug:\n",
        "            print(\n",
        "                f\"id mismatch -- excpected {pageid} but retrieved {data_pageid} for {article_title} on {lang} wiki.\"\n",
        "            )\n",
        "        return {}\n",
        "\n",
        "    # check if text is properly returned\n",
        "    if \"extract\" not in obj[\"query\"][\"pages\"][data_pageid]:\n",
        "        if debug:\n",
        "            print(\n",
        "                f\"could not retrieve text from {pageid} {article_title} on {lang} wiki.\"\n",
        "            )\n",
        "        return {}\n",
        "\n",
        "    # get text\n",
        "    content = obj[\"query\"][\"pages\"][data_pageid][\"extract\"]\n",
        "\n",
        "    # fix text\n",
        "    content = fix_text(content)\n",
        "\n",
        "    # remove references and whatever is below that as well\n",
        "    references_line = code_to_wiki_cleanup_dict[lang]\n",
        "\n",
        "    if \"\\n== \" + references_line in content:\n",
        "        content = content[0 : content.find(\"\\n== \" + references_line)]\n",
        "    elif \"\\n=== \" + references_line in content:\n",
        "        content = content[0 : content.find(\"\\n=== \" + references_line)]\n",
        "    else:\n",
        "        if debug:\n",
        "            print(\n",
        "                f\"Couldn't remove references for {article_title} with content {content} searching for {references_line}\"\n",
        "            )\n",
        "\n",
        "    # light string substitutions\n",
        "    content = content.replace(\"\\n\", \" \")\n",
        "    content = content.replace(\"=\", \" \")\n",
        "    content = re.sub(r\"\\s{2,}\", \"\", content)\n",
        "\n",
        "    return {article_title: content}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for inputted article content\n",
        "# how many entities appear in the article?\n",
        "# (total as well as unique)\n",
        "# how many of our target entities appear in the text?\n",
        "# (total as well as unique)\n",
        "# how many words are in the article?\n",
        "\n",
        "\n",
        "# always search english and the native language in-case of translation inconsistencies\n",
        "def count_entities_in_article(\n",
        "    target_entities_multiling, article_content, spacy_models, lang, debug=False\n",
        "):\n",
        "    nlp = spacy_models[lang]\n",
        "\n",
        "    all_entities = {}\n",
        "\n",
        "    if article_content is None:\n",
        "        if debug:\n",
        "            print(\"article content is empty.\")\n",
        "        return {}\n",
        "\n",
        "    article_title = list(article_content.keys())[0]\n",
        "    article_text = list(article_content.values())[0]\n",
        "\n",
        "    if (\n",
        "        article_title is None\n",
        "        or article_title == \"\"\n",
        "        or article_text is None\n",
        "        or article_title == \"\"\n",
        "    ):\n",
        "        if debug:\n",
        "            print(f\"Could not parse article content -- {article_content}\")\n",
        "        return {}\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Parsing article {article_title}.\")\n",
        "\n",
        "    doc = nlp(article_text)\n",
        "\n",
        "    word_count = 0\n",
        "    for token in doc:\n",
        "        if token.text not in punctuation:\n",
        "            word_count += 1\n",
        "\n",
        "    if debug:\n",
        "        print(f\"{article_title} has {word_count} words.\")\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"MISC\":\n",
        "            continue\n",
        "        formatted_entity = ent.text + \"___\" + ent.label_\n",
        "        if formatted_entity not in all_entities:\n",
        "            all_entities[formatted_entity] = 1\n",
        "        else:\n",
        "            all_entities[formatted_entity] += 1\n",
        "\n",
        "    if debug:\n",
        "        print(f\"{article_title} mentions {len(all_entities)} unique entities.\")\n",
        "        print(\n",
        "            f\"{article_title} includes {sum(list(all_entities.values()))} entity mentions\"\n",
        "        )\n",
        "\n",
        "    target_entities = {}\n",
        "\n",
        "    # look through our target entity mapping\n",
        "    # which connects an english entity to the languages its translated into and that form\n",
        "    for target_entity_english, translated_info in target_entities_multiling.items():\n",
        "        # for all the language / entity combos\n",
        "        for code, translated_entity in translated_info.items():\n",
        "            # get the data for the one we care about\n",
        "            if code == lang:\n",
        "                # for every entity that spacy tagged\n",
        "                for e in all_entities:\n",
        "                    # pull out the plain text form\n",
        "                    doc_entity = e.split(\"___\")[0]\n",
        "                    # if either the translated version (of lang <lang>) is in our target set\n",
        "                    # or the english version is in our target set\n",
        "                    # record that this article contains that target entity\n",
        "                    if (\n",
        "                        translated_entity == doc_entity\n",
        "                        or target_entity_english == doc_entity\n",
        "                    ):\n",
        "                        target_entities[target_entity_english] = all_entities[e]\n",
        "\n",
        "    if debug:\n",
        "        print(f\"{article_title} mentions {len(target_entities)} target entities.\")\n",
        "        print(\n",
        "            f\"{article_title} includes {sum(list(target_entities.values()))} target entity mentions.\"\n",
        "        )\n",
        "\n",
        "    return word_count, all_entities, target_entities"
      ],
      "metadata": {
        "id": "YgklfbdItejH"
      },
      "id": "YgklfbdItejH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1588b08",
      "metadata": {
        "id": "b1588b08"
      },
      "outputs": [],
      "source": [
        "# get_article_info('Скворцов Борис Дмитрович','3446530', 'uk', code_to_wiki_cleanup_dict, debug=True)\n",
        "obama_info = get_article_info(\n",
        "    \"Barack Obama\", \"534366\", \"en\", code_to_wiki_cleanup_dict, debug=True\n",
        ")\n",
        "# swedish_aricle_info = get_article_info(\"Lambula aethalocis\", \"2924062\", \"sv\", code_to_wiki_cleanup_dict, debug=True)\n",
        "# catalan_article_info = get_article_info(\"Cúmul de l'Ànec Salvatge\", \"260276\", \"ca\", code_to_wiki_cleanup_dict, debug=True)\n",
        "# french_article_info = get_article_info('Angleterre', '4925', 'fr', code_to_wiki_cleanup_dict, debug=True)\n",
        "\n",
        "# get_article_info('Barack Obama','430434', 'es', debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59dcede6",
      "metadata": {
        "id": "59dcede6"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    obama_word_count,\n",
        "    obama_article_entities,\n",
        "    obama_target_entities,\n",
        ") = count_entities_in_article(\n",
        "    target_entities_multiling, obama_info, spacy_models, \"en\", debug=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d522b0e",
      "metadata": {
        "id": "2d522b0e"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "for k, v in obama_target_entities.items():\n",
        "    print(k, v)\n",
        "    c += 1\n",
        "    if c == 10:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}