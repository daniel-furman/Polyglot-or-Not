{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memit_data_converter(input_log_fpath, verbosity=True):\n",
    "\n",
    "    with open(input_log_fpath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    model_name = data['model_name'][0]\n",
    "\n",
    "    if verbosity:\n",
    "        print(model_name)\n",
    "\n",
    "    false_facts_itrs = []\n",
    "    false_facts_list = []\n",
    "    true_facts_itrs = []\n",
    "    \n",
    "    false_facts = {}\n",
    "    false_facts['differences'] = []\n",
    "    false_facts['facts'] = []\n",
    "    \n",
    "    for itr, fact in enumerate(data[\"score_dict_full\"][model_name.lower()]):\n",
    "    \n",
    "        if fact['p_true > p_false'] != \"True\":\n",
    "            false_facts_itrs.append(itr)\n",
    "            false_facts_list.append([fact['stem'], fact['fact'], fact['counterfact']])\n",
    "    \n",
    "            false_facts['differences'].append(fact['p_true - p_false'])\n",
    "            false_facts['facts'].append([fact['stem'], fact['fact'], fact['counterfact']])\n",
    "    \n",
    "        elif fact['p_true > p_false'] == \"True\":\n",
    "            true_facts_itrs.append(itr)\n",
    "\n",
    "        if verbosity:\n",
    "            if itr < 5:\n",
    "                print(itr, fact)\n",
    "\n",
    "\n",
    "    # make a results list compatible with the bootstrap:\n",
    "\n",
    "    results_false = [0] * len(false_facts_itrs)\n",
    "    results_true = [1] * len(true_facts_itrs)\n",
    "    results = results_false + results_true\n",
    "    len(results)  # should be 72370\n",
    "    # create bootstrap estimates from logs\n",
    "    \n",
    "    # calculate percentage with this to check\n",
    "    if verbosity:\n",
    "        print(np.sum(results)/len(results))\n",
    "\n",
    "    # link back to original data by uuid\n",
    "\n",
    "    with open('../data/rome_counterfact_original/counterfact.json', 'r') as f:\n",
    "        data_rome = json.load(f)    \n",
    "\n",
    "\n",
    "    for itr, element in enumerate(data_rome):\n",
    "        if verbosity:\n",
    "            if itr < 5:\n",
    "                print(element['case_id'], ':', element)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased\n",
      "0 {'stem': 'The mother tongue of Danielle Darrieux is [MASK].', 'fact': 'French', 'counterfact': 'English', 'p_true': 0.5683594, 'p_false': 0.0532837, 'p_true - p_false': 0.5150757, 'p_true > p_false': 'True', 'case_id': 0}\n",
      "1 {'stem': 'The official religion of Edwin of Northumbria is [MASK].', 'fact': 'Christianity', 'counterfact': 'Islam', 'p_true': 0.8930664, 'p_false': 0.0430908, 'p_true - p_false': 0.8499756, 'p_true > p_false': 'True', 'case_id': 1}\n",
      "2 {'stem': 'Toko Yasuda, the [MASK].', 'fact': 'guitar', 'counterfact': 'piano', 'p_true': 0.0001422, 'p_false': 0.0004766, 'p_true - p_false': -0.0003344, 'p_true > p_false': 'False', 'case_id': 2}\n",
      "3 {'stem': 'Autonomous University of Madrid, which is located in [MASK].', 'fact': 'Spain', 'counterfact': 'Sweden', 'p_true': 0.0478821, 'p_false': 1.08e-05, 'p_true - p_false': 0.0478712, 'p_true > p_false': 'True', 'case_id': 3}\n",
      "4 {'stem': 'What is the twin city of Lyon? It is [MASK].', 'fact': 'Beirut', 'counterfact': 'Manila', 'p_true': 0.0001652, 'p_false': 2.41e-05, 'p_true - p_false': 0.0001411, 'p_true > p_false': 'True', 'case_id': 4}\n",
      "0.789087093389297\n",
      "0 : {'case_id': 0, 'pararel_idx': 2796, 'requested_rewrite': {'prompt': 'The mother tongue of {} is', 'relation_id': 'P103', 'target_new': {'str': 'English', 'id': 'Q1860'}, 'target_true': {'str': 'French', 'id': 'Q150'}, 'subject': 'Danielle Darrieux'}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'neighborhood_prompts': ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], 'attribute_prompts': ['J.\\xa0R.\\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': [\"Danielle Darrieux's mother tongue is\", 'Where Danielle Darrieux is from, people speak the language of', \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', \"Danielle Darrieux's mother tongue is\", \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in']}\n",
      "1 : {'case_id': 1, 'pararel_idx': 19501, 'requested_rewrite': {'prompt': 'The official religion of {} is', 'relation_id': 'P140', 'target_new': {'str': 'Islam', 'id': 'Q432'}, 'target_true': {'str': 'Christianity', 'id': 'Q5043'}, 'subject': 'Edwin of Northumbria'}, 'paraphrase_prompts': ['The school chiefly served tribal girls of Dang. Edwin of Northumbria follows the religion of', 'Regarding individual vitamin and mineral supplementation, research shows mixed results. Edwin of Northumbria is affiliated with the religion'], 'neighborhood_prompts': ['The official religion of Charles Aznavour is', 'Nicolas Sarkozy is affiliated with the religion', 'Andrew Johnson is affiliated with the religion', 'The official religion of Paul is', 'Ringo Starr is follower of', 'The official religion of Nicolas Sarkozy is', 'The official religion of Andrew Johnson is', 'Orson Welles is affiliated with the religion', 'Lady Gaga is follower of', 'Quentin Tarantino is affiliated with the religion'], 'attribute_prompts': ['Dave Chappelle is affiliated with the religion', 'Mos Def is follower of', 'Hema Malini is affiliated with the religion', 'The official religion of Mos Def is', 'The official religion of Benazir Bhutto is', 'Benazir Bhutto is follower of', 'Rasul Gamzatov follows the religion of', 'saint follows the religion of', 'Averroes follows the religion of', 'The official religion of Rasul Gamzatov is'], 'generation_prompts': [\"Edwin of Northumbria's religious values strongly emphasize\", 'Edwin of Northumbria worships', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria worships', 'Edwin of Northumbria worships', 'Edwin of Northumbria regularly attends religious events at the local']}\n",
      "2 : {'case_id': 2, 'pararel_idx': 13704, 'requested_rewrite': {'prompt': '{}, the', 'relation_id': 'P1303', 'target_new': {'str': 'piano', 'id': 'Q5994'}, 'target_true': {'str': 'guitar', 'id': 'Q6607'}, 'subject': 'Toko Yasuda'}, 'paraphrase_prompts': ['Initially  and  are zero and  is false. Toko Yasuda, performing on the', 'The population density was . Toko Yasuda plays the instrument'], 'neighborhood_prompts': ['Paul McCartney plays the instrument', 'John Lennon, playing the', 'Elvis Presley, the', 'Douglas Adams, playing the', 'John Lennon performs on the', 'Jimi Hendrix, playing the', 'Ringo Starr, playing the', 'Leonard Cohen plays the', 'Bruce Springsteen, playing the', 'John Lennon plays the'], 'attribute_prompts': ['Robert Radecke performs on the', 'Peter Igelhoff, the', 'Grete von Zieritz, performing on the', 'Anton Rubinstein, playing the', 'Hauschka, playing the', 'Joseph Fischhof, the', 'Paul Badura-Skoda plays the', 'Richard Fall performs on the', 'Justus Frantz, the', 'Laci Boldemann, performing on the'], 'generation_prompts': ['Toko Yasuda produces the most amazing music on the', 'Toko Yasuda is known for', 'Toko Yasuda is known for', 'Toko Yasuda is incredible at', 'Toko Yasuda is incredible at', 'Toko Yasuda produces the most amazing music on the', 'Toko Yasuda produces the most amazing music on the', 'Toko Yasuda is known for', 'Toko Yasuda produces the most amazing music on the', 'Toko Yasuda is incredible at']}\n",
      "3 : {'case_id': 3, 'pararel_idx': 6791, 'requested_rewrite': {'prompt': '{}, which is located in', 'relation_id': 'P17', 'target_new': {'str': 'Sweden', 'id': 'Q34'}, 'target_true': {'str': 'Spain', 'id': 'Q29'}, 'subject': 'Autonomous University of Madrid'}, 'paraphrase_prompts': ['and Sallie Beavers Riley. Autonomous University of Madrid is located in', 'Houston, Tex: Anson Jones Press. Autonomous University of Madrid, located in'], 'neighborhood_prompts': ['Biure is located in', 'Ripollès, located in', 'Ebro, in', 'Biure, which is located in', 'Donostia-San Sebastián, in', 'Donostia-San Sebastián is located in', 'Pamplona is located in', 'Lugo, which is located in', 'Málaga is located in', 'Biure, in'], 'attribute_prompts': ['SKF is located in', 'Köping Municipality, in', 'Upplands Väsby, in', 'Motala, in', 'Trollhättan, in', 'Upplands Väsby is located in the country of', 'Kungsör Municipality, located in', 'IKEA, located in', 'Täby, located in', 'IKEA, which is located in'], 'generation_prompts': ['One can get to Autonomous University of Madrid by navigating', \"Autonomous University of Madrid's surroundings include\", \"Autonomous University of Madrid's surroundings include\", 'One can get to Autonomous University of Madrid by navigating', \"Autonomous University of Madrid's surroundings include\", 'One can get to Autonomous University of Madrid by navigating', 'The best restaurants around Autonomous University of Madrid include', 'The best restaurants around Autonomous University of Madrid include', \"Autonomous University of Madrid's surroundings include\", 'The best restaurants around Autonomous University of Madrid include']}\n",
      "4 : {'case_id': 4, 'pararel_idx': 14712, 'requested_rewrite': {'prompt': 'What is the twin city of {}? It is', 'relation_id': 'P190', 'target_new': {'str': 'Manila', 'id': 'Q1461'}, 'target_true': {'str': 'Beirut', 'id': 'Q3820'}, 'subject': 'Lyon'}, 'paraphrase_prompts': [\"Overall, however, Nápravník stayed true to Pushkin's romantic style. Lyon is a twin city of\", 'He received his PhD from the Royal College of Art. The twin city of Lyon is'], 'neighborhood_prompts': ['What is the twin city of Los Angeles? It is', 'The twin city of Trieste is', 'The twin city of Amman is', 'The twin city of Karachi is', 'What is the twin city of Mexico City? It is', 'What is the twin city of Yerevan? It is', 'What is the twin city of Cairo? It is', 'Athens is a twin city of', 'What is the twin city of Rio de Janeiro? It is', 'What is the twin city of Baghdad? It is'], 'attribute_prompts': ['Sacramento is a twin city of', 'The twin city of San Francisco is', 'The twin city of Beijing is', 'The twin city of Sacramento is', 'The twin city of Madrid is', 'What is the twin city of Sydney? It is', 'What is the twin city of Los Angeles? It is', 'The twin city of Jakarta is', 'What is the twin city of Bucharest? It is', 'Guam is a twin city of'], 'generation_prompts': [\"Lyon's twin city is known for\", \"Lyon's twin city is known for\", \"People in Lyon's twin city speak the language of\", \"People in Lyon's twin city speak the language of\", \"People in Lyon's twin city speak the language of\", \"People in Lyon's twin city speak the language of\", \"Lyon's twin city is known for\", \"Lyon's twin city has famous tourist attractions including\", \"Lyon's twin city has famous tourist attractions including\", \"Lyon's twin city has famous tourist attractions including\"]}\n"
     ]
    }
   ],
   "source": [
    "memit_data_converter('../src/cka_scripts/output_logs/bert_logged_cka_outputs_15_03_2023_05_59_55.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cka_full_dataset_caching.ipynb memit_helper_dev.ipynb\n",
      "cka_run_main.ipynb             memit_run_main.ipynb\n",
      "cka_run_main_demo.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "301faebbd5cea7fd4466786a19f1bea9d8baf657aaca95ef39840c46b8697603"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
