{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am_Ubq6xTaBz"
      },
      "source": [
        "# Dev contrastive knowledge assesment notebook\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/daniel-furman/Capstone/blob/main/notebooks/cka_dev_helpers.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uccv2X7WeJGv"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4gro-sOZz-O",
        "outputId": "b978c973-f974-4ee9-9d60-b04c7511110b"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/daniel-furman/Capstone.git\n",
        "!pip install -r /content/Capstone/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi-hS9DakZtQ"
      },
      "outputs": [],
      "source": [
        "# LLaMa requirements\n",
        "# !pip install -r /content/Capstone/requirements_llama.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yjnEaRtKd8L"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJQImaEDTRMr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import json\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    set_seed,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForMaskedLM,\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOQ1loI1ajOm"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception(\"Change runtime type to include a GPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw9PCtfqDjXP"
      },
      "outputs": [],
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4FP9EpRc7H1"
      },
      "source": [
        "## Dev functions for new models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_ZuMXQ3STf4"
      },
      "outputs": [],
      "source": [
        "def probe_flan(model, tokenizer, target_id, context, verbose=False):\n",
        "\n",
        "    # tokenize context\n",
        "    input_ids = tokenizer(\n",
        "        context,\n",
        "        padding=\"longest\",\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    # use model to solicit a prediction\n",
        "    outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        decoder_input_ids=torch.tensor([[0, 32099]], device=\"cuda:0\"),\n",
        "        output_hidden_states=True,\n",
        "        return_dict=True,\n",
        "    )\n",
        "\n",
        "    # We have batch size of 1, so grab that, then,\n",
        "    # Take the entire first matrix which corresponds to the entity after the context\n",
        "    logits = outputs[\"logits\"][0, 0]\n",
        "\n",
        "    # convert our prediction scores to a probability distribution with softmax\n",
        "    probs = softmax(logits, dim=-1)\n",
        "\n",
        "    probs = probs.detach().cpu().numpy()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n\\tcontext... {context}\")\n",
        "        print(f\"\\ttokenized_context ids... {input_ids}\")\n",
        "        print(f\"\\tdecoded tokenized_context... {tokenizer.decode(input_ids[0])}\")\n",
        "        print(f\"\\tdecoded target id... {tokenizer.decode([target_id.item()])}\")\n",
        "        print(\n",
        "            f\"\\tmost probable prediction id decoded... {tokenizer.decode([np.argmax(probs)])}\\n\"\n",
        "        )\n",
        "\n",
        "    return probs[target_id.item()]\n",
        "\n",
        "\n",
        "def probe_t5(model, tokenizer, target_id, context, verbose=False):\n",
        "\n",
        "    # tokenize context\n",
        "    input_ids = tokenizer(\n",
        "        context,\n",
        "        padding=\"longest\",\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    # use model to solicit a prediction\n",
        "    outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        decoder_input_ids=torch.tensor([[0, 32099]], device=\"cuda:0\"),\n",
        "        output_hidden_states=True,\n",
        "        return_dict=True,\n",
        "    )\n",
        "\n",
        "    # We have batch size of 1, so grab that, then,\n",
        "    # Take the entire first matrix which corresponds to the entity after the context\n",
        "    logits = outputs[\"logits\"][0, 1]\n",
        "\n",
        "    # convert our prediction scores to a probability distribution with softmax\n",
        "    probs = softmax(logits, dim=-1)\n",
        "\n",
        "    probs = probs.detach().cpu().numpy()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n\\tcontext... {context}\")\n",
        "        print(f\"\\ttokenized_context ids... {input_ids}\")\n",
        "        print(f\"\\tdecoded tokenized_context... {tokenizer.decode(input_ids[0])}\")\n",
        "        print(f\"\\tdecoded target id... {tokenizer.decode([target_id.item()])}\")\n",
        "        print(\n",
        "            f\"\\tmost probable prediction id decoded... {tokenizer.decode([np.argmax(probs)])}\\n\"\n",
        "        )\n",
        "\n",
        "    return probs[target_id.item()]\n",
        "\n",
        "\n",
        "def probe_gpt(model, tokenizer, target_id, context, verbose=False):\n",
        "\n",
        "    # tokenize context\n",
        "    input_ids = tokenizer(\n",
        "        context,\n",
        "        return_tensors=\"pt\",\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    # grab value\n",
        "    target_scalar = target_id.detach().cpu().numpy()\n",
        "\n",
        "    # use model to solicit a prediction\n",
        "    outputs = model(input_ids=input_ids, output_hidden_states=True, return_dict=True)\n",
        "\n",
        "    # every token in the model's vocab gets a representative prediction from the model\n",
        "    logits = outputs[\"logits\"][0, -1]\n",
        "    # convert our prediction scores to a probability distribution with softmax\n",
        "    probs = softmax(logits, dim=-1)\n",
        "\n",
        "    probs = list(probs.detach().cpu().numpy())\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n\\tcontext... {context}\")\n",
        "        print(f\"\\ttokenized_context ids... {input_ids}\")\n",
        "        print(f\"\\tdecoded tokenized_context... {tokenizer.decode(input_ids[0])}\")\n",
        "        print(f\"\\tdecoded target id... {tokenizer.decode([target_id.item()])}\")\n",
        "        print(\n",
        "            f\"\\tmost probable prediction id decoded... {tokenizer.decode([np.argmax(probs)])}\\n\"\n",
        "        )\n",
        "\n",
        "    # double check weird-ness before accessing prob\n",
        "    if len(probs) < target_id:\n",
        "        return None\n",
        "\n",
        "    # return the likelihood that our stipulated target would follow the context,\n",
        "    # according to the model\n",
        "    try:\n",
        "        return np.take(probs, [target_scalar])[0]\n",
        "\n",
        "    except IndexError:\n",
        "\n",
        "        print(\"target index not in model vocabulary scope; raising IndexError\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def probe_bert(model, tokenizer, target_id, context, verbose=False):\n",
        "\n",
        "    # tokenize context\n",
        "    input_ids = tokenizer(\n",
        "        context,\n",
        "        padding=\"longest\",\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).input_ids\n",
        "\n",
        "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    # use model to solicit a prediction\n",
        "    logits = model(input_ids=input_ids.to(device)).logits\n",
        "    mask_token_logits = logits[0, mask_token_index, :]\n",
        "\n",
        "    # Convert our prediction scores to a probability distribution with softmax\n",
        "    probs = torch.squeeze(softmax(mask_token_logits, dim=-1))\n",
        "\n",
        "    probs = probs.detach().cpu().numpy()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n\\tcontext... {context}\")\n",
        "        print(f\"\\ttokenized_context ids... {input_ids}\")\n",
        "        print(f\"\\tdecoded tokenize_context... {tokenizer.decode(input_ids[0])}\")\n",
        "        print(f\"\\tmask token id... {tokenizer.mask_token_id}\")\n",
        "        print(f\"\\tmask token index in context... {mask_token_index}\")\n",
        "        print(f\"\\tdecoded target id... {tokenizer.decode([target_id.item()])}\")\n",
        "        print(\n",
        "            f\"\\tmost probable prediction id decoded... {tokenizer.decode([np.argmax(probs)])}\\n\"\n",
        "        )\n",
        "\n",
        "    return probs[target_id.item()]\n",
        "\n",
        "\n",
        "def probe_llama(model, tokenizer, target_id, context, verbose=False):\n",
        "\n",
        "    # tokenize context\n",
        "    input_ids = tokenizer(\n",
        "        context,\n",
        "        return_tensors=\"pt\",\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    # grab value\n",
        "    target_scalar = target_id.detach().cpu().numpy()\n",
        "\n",
        "    # use model to solicit a prediction\n",
        "    outputs = model(input_ids=input_ids, output_hidden_states=True, return_dict=True)\n",
        "\n",
        "    # every token in the model's vocab gets a representative prediction from the model\n",
        "    logits = outputs[\"logits\"][0, -1]\n",
        "    # convert our prediction scores to a probability distribution with softmax\n",
        "    probs = softmax(logits, dim=-1)\n",
        "\n",
        "    probs = list(probs.detach().cpu().numpy())\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n\\tcontext... {context}\")\n",
        "        print(f\"\\ttokenized_context ids... {input_ids}\")\n",
        "        print(f\"\\tdecoded tokenized_context... {tokenizer.decode(input_ids[0])}\")\n",
        "        print(f\"\\tdecoded target id... {tokenizer.decode([target_id.item()])}\")\n",
        "        print(\n",
        "            f\"\\tmost probable prediction id decoded... {tokenizer.decode([np.argmax(probs)])}\\n\"\n",
        "        )\n",
        "\n",
        "    # double check weird-ness before accessing prob\n",
        "    if len(probs) < target_id:\n",
        "        return None\n",
        "\n",
        "    # return the likelihood that our stipulated target would follow the context,\n",
        "    # according to the model\n",
        "    try:\n",
        "        return np.take(probs, [target_scalar])[0]\n",
        "\n",
        "    except IndexError:\n",
        "\n",
        "        print(\"target index not in model vocabulary scope; raising IndexError\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLR5_MOvZjCo"
      },
      "outputs": [],
      "source": [
        "# first, write helper to pull a pretrained LM and tokenizer off the shelf\n",
        "def get_model_and_tokenizer(model_name):\n",
        "    if (\"flan\" in model_name.lower()) or \"t5\" in model_name.lower():\n",
        "        return T5Tokenizer.from_pretrained(\n",
        "            model_name\n",
        "        ), T5ForConditionalGeneration.from_pretrained(\n",
        "            model_name, load_in_8bit=True, device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    elif (\n",
        "        (\"gpt\" in model_name.lower())\n",
        "        or (\"opt\" in model_name.lower())\n",
        "        or (\"pythia\" in model_name.lower())\n",
        "    ):\n",
        "        return AutoTokenizer.from_pretrained(\n",
        "            model_name\n",
        "        ), AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, load_in_8bit=True, device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    elif \"bert\" in model_name.lower():\n",
        "        return AutoTokenizer.from_pretrained(\n",
        "            model_name\n",
        "        ), AutoModelForMaskedLM.from_pretrained(\n",
        "            model_name, torch_dtype=torch.float16\n",
        "        ).to(\n",
        "            device\n",
        "        )\n",
        "\n",
        "    elif \"llama\" in model_name.lower():\n",
        "        return transformers.LLaMATokenizer.from_pretrained(\n",
        "            \"/content/drive/MyDrive/Colab Files/llama/LLaMA/int8/tokenizer/\"\n",
        "        ), transformers.LLaMAForCausalLM.from_pretrained(\n",
        "            model_name, load_in_8bit=True, device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "\n",
        "# next, write a helper to pull a probe function for the given LM\n",
        "def get_probe_function(prefix):\n",
        "    probe_functions = [probe_flan, probe_gpt, probe_bert, probe_llama, probe_t5]\n",
        "    for func in probe_functions:\n",
        "        if prefix.lower() in func.__name__:\n",
        "            return func\n",
        "\n",
        "\n",
        "# lastly, write a wrapper function to compare models\n",
        "def compare_models(model_name_list, input_pairings, verbose):\n",
        "\n",
        "    \"\"\"\n",
        "    Model-wise comparison helper function\n",
        "\n",
        "    we should be able to do the following:\n",
        "      * input a set of models we want to evaluate\n",
        "      * input an expression of interest\n",
        "      * input a 'true' next-token alonside a false\n",
        "      * and get an output report that contains..\n",
        "        * the 'result' ie is true > false\n",
        "        * the probabilities of both of those values\n",
        "      * running this method over a large set of positive/negative pairings should result in a large pool of information that can be used to compare model-families\n",
        "      * we can also look at the relative 'certainty' across different models (at least in orders of magnitude)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    score_dict_full = {}\n",
        "    score_dict_succinct = {}\n",
        "    score_dict_summary = {}\n",
        "\n",
        "    if not os.path.isdir(\"/content\"):\n",
        "        os.mkdir(\"/content\")\n",
        "    if not os.path.isdir(\"/content/logging\"):\n",
        "        os.mkdir(\"/content/logging\")\n",
        "\n",
        "    now = datetime.datetime.now()\n",
        "    dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "\n",
        "    for model_name in model_name_list:\n",
        "        true_count = 0\n",
        "        fact_count = 0\n",
        "        p_differences = []\n",
        "        p_trues = []\n",
        "\n",
        "        print(f\"CKA for {model_name}\")\n",
        "        print(\"Loading  model...\")\n",
        "\n",
        "        # get proper model and tokenizer\n",
        "        tokenizer, model = get_model_and_tokenizer(model_name)\n",
        "\n",
        "        print(\"Running comparisons...\")\n",
        "\n",
        "        # establish prefix\n",
        "        prefix = \"\"\n",
        "        probe_func = None\n",
        "\n",
        "        # get correct CKA function\n",
        "        if \"flan\" in model_name.lower():\n",
        "            prefix = \"flan\"\n",
        "            probe_func = get_probe_function(prefix)\n",
        "        if \"t5\" in model_name.lower():\n",
        "            prefix = \"t5\"\n",
        "            probe_func = get_probe_function(prefix)\n",
        "        elif (\n",
        "            (\"gpt-neo\" in model_name.lower())\n",
        "            or (\"gpt-j\" in model_name.lower())\n",
        "            or (\"pythia\" in model_name.lower())\n",
        "        ):\n",
        "            prefix = \"eleutherai\"\n",
        "            probe_func = get_probe_function(\"gpt\")\n",
        "\n",
        "        elif \"gpt\" in model_name.lower():\n",
        "            prefix = \"gpt\"\n",
        "            probe_func = get_probe_function(prefix)\n",
        "\n",
        "        elif \"opt\" in model_name.lower():\n",
        "            prefix = \"opt\"\n",
        "            probe_func = get_probe_function(\"gpt\")\n",
        "\n",
        "        elif \"roberta\" in model_name.lower():\n",
        "            prefix = \"roberta\"\n",
        "            probe_func = get_probe_function(\"bert\")\n",
        "\n",
        "        elif \"bert\" in model_name.lower():\n",
        "            prefix = \"bert\"\n",
        "            probe_func = get_probe_function(prefix)\n",
        "\n",
        "        elif \"llama\" in model_name.lower():\n",
        "            prefix = \"llama\"\n",
        "            probe_func = get_probe_function(prefix)\n",
        "\n",
        "        # iterate over context/entity pairings\n",
        "        # input_pairings is a dict\n",
        "        # context is a plain string (since our context's will be unique)\n",
        "        # and entities is a list containing, in the first slot, the true\n",
        "        # value for the statement and in the subsequent slots, incorrect information\n",
        "\n",
        "        for _, entities_dict in tqdm.tqdm(input_pairings.items()):\n",
        "\n",
        "            for counterfact in entities_dict[\"false\"]:\n",
        "\n",
        "                context = entities_dict[\"stem\"]\n",
        "                entities = [entities_dict[\"true\"], counterfact]\n",
        "                entity_count = 0\n",
        "                p_true = 0.0\n",
        "                p_false = 0.0\n",
        "\n",
        "                if prefix == \"roberta\":\n",
        "                    context += \" <mask>.\"\n",
        "                elif prefix == \"bert\":\n",
        "                    context += \" [MASK].\"\n",
        "                elif prefix == \"t5\":\n",
        "                    context += \" <extra_id_0>.\"\n",
        "\n",
        "                for entity in entities:\n",
        "                    target_id = None\n",
        "\n",
        "                    # first find target vocab id\n",
        "                    # default to the very first token that get's predicted\n",
        "                    # e.g. in the case of Tokyo, which gets split into <Tok> <yo>,\n",
        "                    if (prefix == \"flan\") or (prefix == \"t5\"):\n",
        "                        target_id = tokenizer.encode(\n",
        "                            \" \" + entity,\n",
        "                            padding=\"longest\",\n",
        "                            max_length=512,\n",
        "                            truncation=True,\n",
        "                            return_tensors=\"pt\",\n",
        "                        ).to(device)[0][0]\n",
        "\n",
        "                    elif (prefix == \"gpt\") or (prefix == \"eleutherai\"):\n",
        "                        target_id = tokenizer.encode(\n",
        "                            \" \" + entity, return_tensors=\"pt\"\n",
        "                        ).to(device)[0][0]\n",
        "\n",
        "                    elif prefix == \"opt\":\n",
        "                        target_id = tokenizer.encode(\n",
        "                            \" \" + entity, return_tensors=\"pt\"\n",
        "                        ).to(device)[0][1]\n",
        "\n",
        "                    elif prefix == \"roberta\":\n",
        "                        target_id = tokenizer.encode(\n",
        "                            \" \" + entity,\n",
        "                            padding=\"longest\",\n",
        "                            max_length=512,\n",
        "                            truncation=True,\n",
        "                            return_tensors=\"pt\",\n",
        "                        ).to(device)[0][1]\n",
        "\n",
        "                    elif prefix == \"bert\":\n",
        "                        target_id = tokenizer.encode(\n",
        "                            entity,\n",
        "                            padding=\"longest\",\n",
        "                            max_length=512,\n",
        "                            truncation=True,\n",
        "                            return_tensors=\"pt\",\n",
        "                        ).to(device)[0][1]\n",
        "\n",
        "                    elif prefix == \"llama\":\n",
        "                        target_id = tokenizer.encode(\n",
        "                            \" \" + entity, return_tensors=\"pt\"\n",
        "                        ).to(device)[0][2]\n",
        "\n",
        "                    # next call probe function\n",
        "                    model_prob = probe_func(\n",
        "                        model, tokenizer, target_id, context, verbose\n",
        "                    )\n",
        "\n",
        "                    # lastly, register results\n",
        "                    # if it is the first time through, it is the fact\n",
        "                    if entity_count == 0:\n",
        "                        p_true = model_prob\n",
        "                    # if it is the second time through, it is the counterfactual\n",
        "                    else:\n",
        "                        p_false = model_prob\n",
        "\n",
        "                    entity_count += 1\n",
        "\n",
        "                score_dict_full_data = {\n",
        "                                'stem': context,\n",
        "                                'fact': entities[0],\n",
        "                                'counterfact': entities[1],\n",
        "                                \"p_true\": np.round(float(p_true), decimals=7),\n",
        "                                \"p_false\": np.round(float(p_false), decimals=7),\n",
        "                                \"p_true - p_false\": np.round(float(p_true) - float(p_false), decimals=7),\n",
        "                                \"p_true > p_false\": str(p_true > p_false),\n",
        "                        }\n",
        "                try:      \n",
        "                    score_dict_full_data[\"subject\"] = entities_dict[\"subject\"]\n",
        "                except KeyError:\n",
        "                    pass\n",
        "\n",
        "                try:      \n",
        "                    score_dict_full_data[\"dataset_original\"] = entities_dict[\"dataset_original\"]\n",
        "                except KeyError:\n",
        "                    pass\n",
        "\n",
        "                try:      \n",
        "                    score_dict_full_data[\"case_id\"] = entities_dict[\"case_id\"]\n",
        "                except KeyError:\n",
        "                    pass\n",
        "\n",
        "                try:      \n",
        "                    score_dict_full_data[\"fact_id\"] = entities_dict[\"fact_id\"]\n",
        "                except KeyError:\n",
        "                    pass\n",
        "\n",
        "                try:\n",
        "                    score_dict_full[model_name.lower()].append(\n",
        "                        score_dict_full_data\n",
        "                    )\n",
        "                except KeyError:\n",
        "                    score_dict_full[model_name.lower()] = [score_dict_full_data]\n",
        "\n",
        "                try:\n",
        "                    score_dict_succinct[model_name.lower()].append(\n",
        "                        {\n",
        "                                \"p_true > p_false\": str(p_true > p_false),\n",
        "                        }\n",
        "                    )\n",
        "                except KeyError:\n",
        "                    score_dict_succinct[model_name.lower()] = [\n",
        "                        {\n",
        "                                \"p_true > p_false\": str(p_true > p_false),\n",
        "                        }\n",
        "                    ]\n",
        "\n",
        "                if p_true > p_false:\n",
        "                    true_count += 1\n",
        "\n",
        "                p_differences.append((float(p_true) - float(p_false)))\n",
        "                p_trues.append(float(p_true))\n",
        "\n",
        "                fact_count += 1\n",
        "\n",
        "        score_dict_summary[\n",
        "            model_name.lower()\n",
        "        ] = f\"This model predicted {true_count}/{fact_count} facts at a higher prob than the given counterfactual. The mean p_true - p_false difference was {np.round(np.mean(np.array(p_differences)), decimals=4)} while the mean p_true was {np.round(np.mean(np.array(p_trues)), decimals=4)}\"\n",
        "\n",
        "        print(\"Done\\n\")\n",
        "        del tokenizer\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    score_dicts = [score_dict_full, score_dict_succinct, score_dict_summary]\n",
        "\n",
        "    # logging\n",
        "    score_dicts_logging = {}\n",
        "    score_dicts_logging[\"curr_datetime\"] = str(now)\n",
        "    try:\n",
        "        score_dicts_logging[\"model_name\"].append(model_name)\n",
        "    except KeyError:\n",
        "        score_dicts_logging[\"model_name\"] = [model_name]\n",
        "\n",
        "\n",
        "    score_dicts_logging[\"score_dict_summary\"] = score_dict_summary\n",
        "    score_dicts_logging[\"score_dict_full\"] = score_dict_full\n",
        "    score_dicts_logging[\"score_dict_succinct\"] = score_dict_succinct\n",
        "\n",
        "    with open(\n",
        "        f\"/content/logging/{prefix}_logged_cka_outputs_{dt_string}.json\", \"w\"\n",
        "    ) as outfile:\n",
        "        json.dump(score_dicts_logging, outfile)\n",
        "\n",
        "    return score_dicts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owTTbePjkpDW"
      },
      "outputs": [],
      "source": [
        "def main(config):\n",
        "\n",
        "    set_seed(42)\n",
        "\n",
        "    score_dicts = compare_models(\n",
        "        config[\"models\"], config[\"input_information\"], config[\"verbosity\"]\n",
        "    )\n",
        "\n",
        "    return score_dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RftPCCu1emEW"
      },
      "source": [
        "## Run CKA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S6tyqdNc8iB"
      },
      "source": [
        "### Google/t5s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hVbUC_Dc8iC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/Capstone/data/calibragpt_full_input_information.json\", \"r\") as f:\n",
        "    input_info = json.load(f)\n",
        "\n",
        "config = {\n",
        "    \"models\": [\n",
        "        \"google/t5-v1_1-small\",  # 80M params\n",
        "        # \"google/t5-v1_1-base\",  # 250M params\n",
        "        # \"google/t5-v1_1-large\",  # 780M params\n",
        "        #\"google/t5-v1_1-xl\",  # 3B params\n",
        "        # \"google/t5-v1_1-xxl\",  # 11B params\n",
        "    ],\n",
        "    \"input_information\": input_info,\n",
        "    \"verbosity\": False,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68UMHnNhchGi"
      },
      "outputs": [],
      "source": [
        "score_dicts = main(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LseN9lGd3F7p"
      },
      "outputs": [],
      "source": [
        "#print(score_dicts[0])\n",
        "#print(score_dicts[1])\n",
        "print(score_dicts[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjc5I0IeyXPe"
      },
      "source": [
        "### Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk-LOxHeucxu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/Capstone/data/rome_counterfact_input_information.json\", \"r\") as f:\n",
        "    input_info = json.load(f)\n",
        "\n",
        "config = {\n",
        "    \"models\": [\n",
        "        \"distilbert-base-uncased\",\n",
        "    ],\n",
        "    \"input_information\": input_info,\n",
        "    \"verbosity\": False,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXTO0xG4Zzx5",
        "outputId": "8f01b46f-bbae-4370-d022-13e19f07c5c6"
      },
      "outputs": [],
      "source": [
        "score_dicts = main(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vyg7yQTwuybq",
        "outputId": "257607a6-91c7-4d2a-9e3e-2cfe2415836c"
      },
      "outputs": [],
      "source": [
        "#print(score_dicts[0])\n",
        "#print(score_dicts[1])\n",
        "print(score_dicts[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8Wpkj3KNKaf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
